{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore the process of gathering and preprocessing historical daily stock prices and commodities prices, as well as macroeconomic and technical analysis indicators. This data will be used to train several commonly used regression models to predict the variation of the price of a stock for the next day. We will analyze the performance of multiple linear, decision tree, random forest, support vector, xgbost, and catbost regression models, optimized using Bayesian optimization techniques. Additionally, we will validate each model using the time series hold one out cross-validation strategy, and quantify and graph their performances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have them installed here are the packages we will be using\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install sklearn\n",
    "# !pip install tensorflow #\n",
    "# !pip install yfinance\n",
    "# !pip install plotly #\n",
    "# !pip install pandas_ta #\n",
    "# !pip install scikit-optimize # for hyperparameter tuning\n",
    "# !pip install TA-Lib # for technical analysis\n",
    "# !pip install xgboost # \n",
    "# !pip install catboost # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import used libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import pandas_ta as ta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Petrobras data from Yahoo Finance trough pandas_ta\n",
    "df = pd.DataFrame()\n",
    "df = df.ta.ticker(\"petr4.sa\", start='1998-01-01', end='2022-12-27')\n",
    "df.index = df.index.date\n",
    "df.index.name = 'Date'\n",
    "# make index datetime\n",
    "df.index = pd.to_datetime(df.index)\n",
    "# sort by index\n",
    "df = df.sort_index()\n",
    "\n",
    "# get days in which stocks were traded (aka: the index the df)\n",
    "days_total = (pd.DataFrame(df.index)\n",
    "                .reset_index(drop=False)\n",
    "                .set_index('Date')\n",
    "                .drop(columns=['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "def apply_delay(s, forwarddelay):\n",
    "    s_copy = s.copy()\n",
    "    s_copy = s_copy.shift(forwarddelay)\n",
    "    return s_copy\n",
    "\n",
    "# adder of a few lags to the dataframe\n",
    "def get_data_w_some_lags(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create columns of returns of Open, High, Low, Close\n",
    "    for i in range(4):\n",
    "        df[df.columns[i] + '_pct_change'] = df[df.columns[i]].pct_change(fill_method=None)\n",
    "\n",
    "    # forwarddelay = 1, 2\n",
    "    for lag in [1, 2]:\n",
    "        for col in ['Open', 'High', 'Low', 'Close']:\n",
    "            df[col + '_pct_change_lag_f' + str(lag)] = apply_delay(df[col + '_pct_change'], lag)\n",
    "\n",
    "    # make the return of the last lag days a feature\n",
    "    for lag in [2, 5, 10]:\n",
    "        df['Close_cum_pct_change_from_' + str(lag)] = df['Close'].pct_change(periods=lag, fill_method=None)\n",
    "\n",
    "    # make the weekly return a feature for the last 3 weeks (the first we already have)\n",
    "    for lag in [2, 3]:\n",
    "        df['Close_cum_pct_change_from_' + str(lag) + '_week'] = (df['Close'].shift((lag-1)*5)\n",
    "                                                               / df['Close'].shift(lag*5)\n",
    "                                                               -1)\n",
    "    \n",
    "    # make the monthly return a feature for the last 2 months\n",
    "    for lag in [1, 2]:\n",
    "        df['Close_cum_pct_change_from_' + str(lag) + '_month'] = (df['Close'].shift((lag-1)*20)\n",
    "                                                                / df['Close'].shift(lag*20)\n",
    "                                                                - 1)\n",
    "\n",
    "    # make sma of 3 days for 'Volume' as a feature\n",
    "    if 'Volume' in df.columns:\n",
    "        df['Volume_sma_3'] = df['Volume'].rolling(3).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "# adder of lags to the dataframe\n",
    "def get_data_w_lags(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create columns of % change of Open, High, Low, Close\n",
    "    for i in range(4):\n",
    "        df[df.columns[i] + '_pct_change'] = df[df.columns[i]].pct_change(fill_method=None)\n",
    "\n",
    "    # forwarddelay = 1, 2, 3, 5, 10, 15, 20\n",
    "    for lag in [1, 2, 3, 5, 10, 15, 20]:\n",
    "        for col in ['Open', 'High', 'Low', 'Close']:\n",
    "            df[col + '_pct_change_lag_f' + str(lag)] = apply_delay(df[col + '_pct_change'], lag)\n",
    "\n",
    "    # make the return of the last lag days a feature\n",
    "    for lag in [2, 3, 5, 10, 15, 20]:\n",
    "        df['Close_cum_pct_change_from_' + str(lag)] = df['Close'].pct_change(periods=lag, fill_method=None)\n",
    "\n",
    "    # make the weekly return a feature for the last 4 weeks (the first we already have)\n",
    "    for lag in [2, 3, 4]:\n",
    "        df['Close_cum_pct_change_from_' + str(lag) + '_week'] = (df['Close'].shift((lag-1)*5)\n",
    "                                                               / df['Close'].shift(lag*5)\n",
    "                                                               -1)\n",
    "    \n",
    "    # make the monthly return a feature for the last 3 months\n",
    "    for lag in [1, 2]:\n",
    "        df['Close_cum_pct_change_from_' + str(lag) + '_month'] = (df['Close'].shift((lag-1)*20)\n",
    "                                                                / df['Close'].shift(lag*20)\n",
    "                                                                - 1)\n",
    "\n",
    "    # make sma of 3 days for 'Volume' as a feature\n",
    "    if 'Volume' in df.columns:\n",
    "        df['Volume_sma_3'] = df['Volume'].rolling(3).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "# add the time lags to the benchmark dataframe\n",
    "def add_time_lags_to_bench(df_bench, days):\n",
    "    df_bench = (days.loc['2000-01-31':]\n",
    "                    .merge(df_bench, how='left', left_index=True, right_index=True)\n",
    "                    .ffill())\n",
    "    \n",
    "    # take Open, High, Low, and Close columns, apply get_data_w_lags function and concatenate the dataframe\n",
    "    df_bench_enlarged = pd.DataFrame()\n",
    "    for col in set([c.split('_')[0] for c in df_bench.columns]):\n",
    "\n",
    "        df = (df_bench.filter(regex=col)\n",
    "                      .rename(columns={f'{col}_Close': 'Close',\n",
    "                                       f'{col}_Open': 'Open',\n",
    "                                       f'{col}_High': 'High',\n",
    "                                       f'{col}_Low': 'Low'}))\n",
    "        if col == 'IBOV':\n",
    "            df = get_data_w_lags(df)\n",
    "            df.columns = [f'{col}_{c}' for c in df.columns]\n",
    "        else:\n",
    "            df = get_data_w_some_lags(df)\n",
    "            if col == 'XAU':\n",
    "                df.columns = [f'GOLD_{c}' for c in df.columns]\n",
    "            else:\n",
    "                df.columns = [f'USDBRL_{c}' for c in df.columns]\n",
    "        df_bench_enlarged = pd.concat([df_bench_enlarged, df], axis=1)\n",
    "    return df_bench_enlarged.loc['2001-01-31':], days.loc['2001-01-31':]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare external features\n",
    "# all data from 2001-01-31 to 2020-12-31\n",
    "df_macro = pd.read_csv('data_raw/Dados_MacroBR_Historico.csv', sep=',', index_col=0, parse_dates=True)\n",
    "df_comm = pd.read_csv('data_raw/Dados_Commodities_Historico.csv', sep=',', index_col=0, parse_dates=True)\n",
    "df_bench = pd.read_csv('data_raw/Dados_Ibov_Dolar_Ouro.csv', sep=',', index_col=0, parse_dates=True)\n",
    "\n",
    "# expand the temporal caracteristics of the benchmark dataframe as new features and make it start at '2001-01-31'\n",
    "df_bench_enlarged, days = add_time_lags_to_bench(df_bench, days_total)\n",
    "df_macro.loc[:'2001-01-31'] = df_macro.loc[:'2001-01-31'].ffill()\n",
    "columns = df_macro.columns[df_macro.loc['2001-01-31'].isna()].tolist()\n",
    "df_macro = df_macro.drop(columns=columns)\n",
    "\n",
    "# merge all dataframes and fix them to the same index (dates in which the market has opened)\n",
    "df_merge = (days.merge(df_macro, how='left', left_index=True, right_index=True)\n",
    "                .merge(df_comm, how='left', left_index=True, right_index=True)\n",
    "                .merge(df_bench_enlarged, how='left', left_index=True, right_index=True))\n",
    "\n",
    "# fill nan values with the last value\n",
    "df_merge = df_merge.ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date: 2001-01-31 00:00:00\n",
      "Last date: 2022-12-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# check the range of the data\n",
    "last_date = np.min([df_merge[col].last_valid_index() for col in df_merge.columns])\n",
    "first_date = np.max([df_merge[col].first_valid_index() for col in df_merge.columns])\n",
    "print(f'First date: {first_date}\\nLast date: {last_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "# input economic rules as features\n",
    "def input_classes(df):\n",
    "    df['AO_5_34_dir'] = [1 if x > 0 else 0 for x in df['AO_5_34']]\n",
    "    df['BOP_dir'] = [1 if x > 0 else 0 for x in df['BOP']]\n",
    "    df['CCI_14_0.015_dir'] = [1 if x > -100 else 0 for x in df['CCI_14_0.015']]\n",
    "    df['CFO_9_dir'] = [1 if x > 0 else 0 for x in df['CFO_9']]\n",
    "    df['CG_10_dir'] = [1 if x > df['CG_10'].shift(1)[i] else 0 for i, x in enumerate(df['CG_10'])]\n",
    "    df['COPC_11_14_10_dir'] = [1 if (x < 0 and x > df['COPC_11_14_10'][i-1]) else 0 for i, x in enumerate(df['COPC_11_14_10'])]\n",
    "    df['CTI_12_dir'] = [1 if x > 0 else 0 for x in df['CTI_12']]\n",
    "    df['DM_14_dir'] = [1 if df['DMP_14'][i] > df['DMN_14'][i] else 0 for i in range(len(df))]\n",
    "    df['FISHERT_9_1_dir'] = [1 if df['FISHERT_9_1'][i] > df['FISHERTs_9_1'][i] else 0 for i in range(len(df))]\n",
    "    df['INERTIA_20_14_dir'] = [1 if x > 50 else 0 for x in df['INERTIA_20_14']]\n",
    "    df['KDJ_9_3_dir'] = [1 if (df['K_9_3'][i] > df['D_9_3'][i] and df['K_9_3'][i] > 20 and df['K_9_3'][i] < 80) else 0 for i in range(len(df))]\n",
    "    df['KST_10_15_20_30_10_10_10_15_dir'] = [1 if (df['K_9_3'][i] > df['D_9_3'][i] and df['K_9_3'][i] > 20 and df['K_9_3'][i] < 80) else 0 for i in range(len(df))]\n",
    "    df['MACD_12_26_9_dir'] = [1 if x > 0 else 0 for x in df['MACDh_12_26_9']]\n",
    "    df['MOM_10_dir'] = [1 if x > 0 else 0 for x in df['MOM_10']]\n",
    "    df['PGO_14_dir'] = [1 if x > 0 else 0 for x in df['PGO_14']]\n",
    "    df['PSL_12_dir'] = [1 if x > 50 else 0 for x in df['PSL_12']]\n",
    "    df['ROC_10_dir'] = [1 if x > 0 else 0 for x in df['ROC_10']]\n",
    "    df['RSI_14_dir'] = [1 if (x > 30 and x < 70) else 0 for x in df['RSI_14']]\n",
    "    df['RSX_14_dir'] = [1 if (x > 30 and x < 70) else 0 for x in df['RSX_14']]\n",
    "    df['RVGI_14_4_dir'] = [1 if df['RVGI_14_4'][i] > df['RVGIs_14_4'][i] else 0 for i in range(len(df))]\n",
    "    df['SMI_5_20_5_dir'] = [1 if x > 0 else 0 for x in df['SMI_5_20_5']]\n",
    "    df['STOCH_14_3_3_dir'] = [1 if df['STOCHk_14_3_3'][i] > df['STOCHd_14_3_3'][i] else 0 for i in range(len(df))]\n",
    "    df['UO_7_14_28_dir'] = [1 if (x > 30 and x < 70) else 0 for x in df['UO_7_14_28']]\n",
    "    df['WILLR_14_dir'] = [1 if (x > -80 and x < -20) else 0 for x in df['WILLR_14']]\n",
    "    df['ADX_14_dir'] = [1 if x > 30 else 0 for x in df['ADX_14']]\n",
    "    df['AROON_14_dir'] = [1 if df['AROONU_14'][i] > df['AROOND_14'][i] else 0 for i in range(len(df))]\n",
    "    df['CKSP_10_3_20_dir'] = [1 if (df['Close'][i] > df['CKSPl_10_3_20'][i] and df['Close'][i] > df['CKSPs_10_3_20'][i]) else 0 for i in range(len(df))]\n",
    "    df['QS_10_dir'] = [1 if x > 0 else 0 for x in df['QS_10']]\n",
    "    df['VHF_28_dir'] = [1 if x > 0.5 else 0.5 for x in df['VHF_28']]\n",
    "    df['VTX_14_dir'] = [1 if df['VTXP_14'][i] > df['VTXM_14'][i] else 0 for i in range(len(df))]\n",
    "    df['MCGD_10_dir'] = [1 if df['Close'][i] > df['MCGD_10'][i] else 0 for i in range(len(df))]\n",
    "    df['VIDYA_14_dir'] = [1 if df['Close'][i] > df['VIDYA_14'][i] else 0 for i in range(len(df))]\n",
    "    return df\n",
    "\n",
    "# feature engineering function\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # get time lags\n",
    "    df = get_data_w_lags(df)\n",
    "\n",
    "    # get technical indicators from the pandas_ta library as features\n",
    "    df.ta.strategy()\n",
    "\n",
    "    # drop the columns that have look-ahead bias or that yeild to many NaNs\n",
    "    df = df.drop(['DPO_20','ICS_26','HILOl_13_21','HILOs_13_21','ISB_26','KVOs_34_55_13','VWAP_D',\n",
    "                    'PSARl_0.02_0.2','PSARs_0.02_0.2','QQE_14_5_4.236','QQEl_14_5_4.236','EOM_14_100000000',\n",
    "                    'QQEs_14_5_4.236','SUPERTl_7_3.0','SUPERTs_7_3.0','TRIX_30_9','TSIs_13_25_13',\n",
    "                    'TRIXs_30_9','ISA_9','KSTs_9','KVO_34_55_13','T3_10_0.7'], axis=1)\n",
    "\n",
    "    # the first 2 months have the majority of the missing values, so we will drop them\n",
    "    df = (df.drop(df.index[:46])\n",
    "            .drop(df.index[-1]))\n",
    "\n",
    "    # # gather the columns that have more than 1% of the data missing and print them to file\n",
    "    # columns_droped = df.columns[df.isna().sum(axis=0) > df.shape[0]*0.01]\n",
    "    # with open('columns_droped.txt', 'a') as f:\n",
    "    #     f.write(str(columns_droped))\n",
    "\n",
    "    df = input_classes(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# fill the infinities of each column with the maximum value of that column\n",
    "def fill_infinities(df):\n",
    "    n_infinities = df.isin([np.inf, -np.inf]).sum(axis=0).sum()\n",
    "    # loop through the columns with infinities\n",
    "    for col in df.columns[df.isin([np.inf, -np.inf]).sum(axis=0) > 0]:\n",
    "        # get the maximum value which is not infinity of the column\n",
    "        max_value = df[col].replace([np.inf, -np.inf], np.nan).max()\n",
    "        min_value = df[col].replace([np.inf, -np.inf], np.nan).min()\n",
    "        # replace the infinities with the maximum value\n",
    "        df[col].replace([np.inf], max_value, inplace=True)\n",
    "        df[col].replace([-np.inf], min_value, inplace=True)\n",
    "    return df, n_infinities\n",
    "\n",
    "# clip outliers by capping and flooring the values to the 4 std from the mean\n",
    "def clip_outliers(df):\n",
    "    n_outliers = {}\n",
    "    for col in df.columns:\n",
    "        # get the mean and std of the column\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        # get the number of outliers\n",
    "        if df[col][df[col] > mean + 4*std].count() + df[col][df[col] < mean - 4*std].count() > 0:\n",
    "            n_outliers[col] = df[col][df[col] > mean + 4*std].count() + df[col][df[col] < mean - 4*std].count()\n",
    "        # cap the outliers\n",
    "        df[col] = df[col].clip(mean - 4*std, mean + 4*std)\n",
    "    # sort the outliers by the number of outliers\n",
    "    n_outliers = {k: v for k, v in sorted(n_outliers.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return df, n_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 infinities filled in the data.\n",
      "Outliers cliped by column: {'CDL_MARUBOZU': 273, 'CDL_3OUTSIDE': 224, 'BZGDQOQ%': 185, 'CDL_HARAMICROSS': 144, 'CDL_HIKKAKE': 133, 'CDL_HANGINGMAN': 124, 'CDL_3INSIDE': 120, 'CDL_DOJISTAR': 117, 'CDL_HAMMER': 104, 'CDL_MATCHINGLOW': 102, 'CDL_HOMINGPIGEON': 95, 'PVOL': 91, 'CDL_SHOOTINGSTAR': 84, 'CDL_INVERTEDHAMMER': 81, 'Volume_sma_3': 78, 'CDL_TAKURI': 77, 'CDL_DRAGONFLYDOJI': 74, 'Volume': 72, 'MAD_30': 66, 'EFI_13': 65, 'BZPIIPMO': 64, 'BZRTRETM': 64, 'STDEV_30': 63, 'VAR_30': 63, 'ADOSC_3_10': 62, 'DMN_14': 62, 'PVOs_12_26_9': 62, 'UI_14': 62, 'CDL_GRAVESTONEDOJI': 61, 'MOM_10': 58, 'SQZ_20_2.0_20_1.5': 58, 'SQZPRO_20_2.0_20_2_1.5_1': 58, 'AO_5_34': 56, 'MACDh_12_26_9': 56, 'PVO_12_26_9': 56, 'APO_12_26': 54, 'MEDIAN_30': 53, 'QTL_30_0.5': 53, 'BCOMNI': 51, 'BEARP_13': 49, 'MACD_12_26_9': 49, 'MACDs_12_26_9': 49, 'STCmacd_10_12_26_0.5': 49, 'ABER_ATR_5_15': 48, 'SLOPE_1': 47, 'ADX_14': 45, 'ATRr_14': 44, 'IKS_26': 44, 'DCU_20_20': 43, 'BZPIIPCM': 43, 'ACCBU_20': 42, 'SQZPRO_ON_NARROW': 42, 'BBB_5_2.0': 40, 'CDL_TASUKIGAP': 40, 'CDL_THRUSTING': 40, 'HWU': 40, 'IBREGPMM': 40, 'DCM_20_20': 39, 'MASSI_9_25': 39, 'BBU_5_2.0': 38, 'High': 37, 'ENTP_10': 37, 'KCUe_20_2': 37, 'USDBRL_High_pct_change': 37, 'USDBRL_High_pct_change_lag_f1': 37, 'USDBRL_High_pct_change_lag_f2': 37, 'ABER_SG_5_15': 36, 'CKSPs_10_3_20': 36, 'KAMA_10_2_30': 36, 'ACCBM_20': 35, 'DMP_14': 35, 'HA_high': 35, 'HL2': 35, 'HWM': 35, 'HWMA_0.2_0.1_0.1': 35, 'ALMA_10_6.0_0.85': 34, 'DEMA_10': 34, 'HA_close': 34, 'JMA_7_0': 34, 'MIDPRICE_2': 34, 'OHLC4': 34, 'PPOh_12_26_9': 34, 'SINWMA_14': 34, 'SSF_10_2': 34, 'SWMA_10': 34, 'TRIMA_10': 34, 'Open': 33, 'ABER_ZG_5_15': 33, 'BBL_5_2.0': 33, 'BBM_5_2.0': 33, 'CDL_GAPSIDESIDEWHITE': 33, 'LDECAY_5': 33, 'EMA_10': 33, 'HA_open': 33, 'HLC3': 33, 'KURT_30': 33, 'LR_14': 33, 'PWMA_10': 33, 'SMA_10': 33, 'TEMA_10': 33, 'ZL_EMA_10': 33, 'USDBRL_Open_pct_change': 33, 'USDBRL_Open_pct_change_lag_f1': 33, 'USDBRL_Open_pct_change_lag_f2': 33, 'GOLD_Low_pct_change': 33, 'GOLD_Low_pct_change_lag_f1': 33, 'GOLD_Low_pct_change_lag_f2': 33, 'Close': 32, 'ABER_XG_5_15': 32, 'CDL_EVENINGSTAR': 32, 'FWMA_10': 32, 'HMA_10': 32, 'HWL': 32, 'PPO_12_26_9': 32, 'PPOs_12_26_9': 32, 'THERMOma_20_2_0.5': 32, 'WMA_10': 32, 'IBOV_High_pct_change': 32, 'IBOV_Low_pct_change': 32, 'IBOV_High_pct_change_lag_f1': 32, 'IBOV_Low_pct_change_lag_f1': 32, 'IBOV_High_pct_change_lag_f2': 32, 'IBOV_Low_pct_change_lag_f2': 32, 'IBOV_High_pct_change_lag_f3': 32, 'IBOV_Low_pct_change_lag_f3': 32, 'IBOV_High_pct_change_lag_f5': 32, 'IBOV_Low_pct_change_lag_f5': 32, 'IBOV_High_pct_change_lag_f10': 32, 'IBOV_Low_pct_change_lag_f10': 32, 'IBOV_High_pct_change_lag_f15': 32, 'IBOV_Low_pct_change_lag_f15': 32, 'Low': 31, 'Close_pct_change_lag_f1': 31, 'Close_pct_change_lag_f2': 31, 'Close_pct_change_lag_f3': 31, 'Close_pct_change_lag_f5': 31, 'Close_pct_change_lag_f10': 31, 'Close_pct_change_lag_f15': 31, 'Close_pct_change_lag_f20': 31, 'Close_cum_pct_change_from_15': 31, 'CDL_DARKCLOUDCOVER': 31, 'CDL_TRISTAR': 31, 'HA_low': 31, 'ITS_9': 31, 'MIDPOINT_2': 31, 'PCTRET_1': 31, 'VWMA_10': 31, 'WCP': 31, 'IBOV_Open_pct_change_lag_f20': 31, 'IBOV_High_pct_change_lag_f20': 31, 'IBOV_Low_pct_change_lag_f20': 31, 'CDL_XSIDEGAP3METHODS': 30, 'CKSPl_10_3_20': 30, 'BCOMCT': 30, 'IBOV_Open_pct_change': 30, 'IBOV_Open_pct_change_lag_f1': 30, 'IBOV_Open_pct_change_lag_f2': 30, 'IBOV_Open_pct_change_lag_f3': 30, 'IBOV_Open_pct_change_lag_f5': 30, 'IBOV_Open_pct_change_lag_f10': 30, 'IBOV_Open_pct_change_lag_f15': 30, 'IBOV_Close_pct_change_lag_f20': 30, 'Low_pct_change': 29, 'Low_pct_change_lag_f1': 29, 'Low_pct_change_lag_f2': 29, 'Low_pct_change_lag_f3': 29, 'Low_pct_change_lag_f5': 29, 'COPC_11_14_10': 29, 'RMA_10': 29, 'IBOV_Close_pct_change': 29, 'IBOV_Close_pct_change_lag_f1': 29, 'IBOV_Close_pct_change_lag_f2': 29, 'IBOV_Close_pct_change_lag_f3': 29, 'IBOV_Close_pct_change_lag_f5': 29, 'IBOV_Close_pct_change_lag_f10': 29, 'IBOV_Close_pct_change_lag_f15': 29, 'Low_pct_change_lag_f10': 28, 'Low_pct_change_lag_f15': 28, 'Low_pct_change_lag_f20': 28, 'Close_cum_pct_change_from_20': 28, 'Close_cum_pct_change_from_1_month': 28, 'Close_cum_pct_change_from_2_month': 28, 'CDL_PIERCING': 28, 'CG_10': 28, 'KST_10_15_20_30_10_10_10_15': 28, 'LOGRET_1': 28, 'USDBRL_Low_pct_change': 28, 'USDBRL_Low_pct_change_lag_f1': 28, 'USDBRL_Low_pct_change_lag_f2': 28, 'BR_26': 27, 'KCBe_20_2': 27, 'IBOV_Close_cum_pct_change_from_3': 27, 'GOLD_High_pct_change': 27, 'GOLD_High_pct_change_lag_f1': 27, 'GOLD_High_pct_change_lag_f2': 27, 'Close_cum_pct_change_from_2': 26, 'CDL_MORNINGSTAR': 26, 'BULLP_13': 26, 'HILO_13_21': 26, 'IBOV_Close_cum_pct_change_from_2': 26, 'Close_cum_pct_change_from_3': 25, 'Close_cum_pct_change_from_10': 25, 'BIAS_SMA_26': 25, 'ROC_10': 25, 'USDBRL_Close_cum_pct_change_from_2': 25, 'IBOV_Close_cum_pct_change_from_5': 25, 'IBOV_Close_cum_pct_change_from_2_week': 25, 'IBOV_Close_cum_pct_change_from_3_week': 25, 'IBOV_Close_cum_pct_change_from_4_week': 25, 'Close_cum_pct_change_from_5': 24, 'Close_cum_pct_change_from_2_week': 24, 'Close_cum_pct_change_from_3_week': 24, 'Close_cum_pct_change_from_4_week': 24, 'CDL_ADVANCEBLOCK': 24, 'CFO_9': 24, 'NATR_14': 24, 'USDBRL_Close_pct_change': 24, 'USDBRL_Close_pct_change_lag_f1': 24, 'USDBRL_Close_pct_change_lag_f2': 24, 'IBOV_Close_cum_pct_change_from_20': 24, 'IBOV_Close_cum_pct_change_from_1_month': 24, 'IBOV_Close_cum_pct_change_from_2_month': 24, 'GOLD_Close_cum_pct_change_from_5': 24, 'GOLD_Close_cum_pct_change_from_2_week': 24, 'GOLD_Close_cum_pct_change_from_3_week': 24, 'ACCBL_20': 23, 'GOLD_Close_pct_change': 23, 'GOLD_Close_pct_change_lag_f1': 23, 'GOLD_Close_pct_change_lag_f2': 23, 'GOLD_Close_cum_pct_change_from_2': 23, 'USDBRL_Close_cum_pct_change_from_5': 22, 'USDBRL_Close_cum_pct_change_from_2_week': 22, 'USDBRL_Close_cum_pct_change_from_3_week': 22, 'IBOV_Close_cum_pct_change_from_15': 22, 'GOLD_Open_pct_change': 22, 'GOLD_Open_pct_change_lag_f1': 22, 'GOLD_Open_pct_change_lag_f2': 22, 'PVOh_12_26_9': 21, 'DCL_20_20': 20, 'SUPERT_7_3.0': 20, 'USDBRL_Close_cum_pct_change_from_1_month': 19, 'USDBRL_Close_cum_pct_change_from_2_month': 19, 'IBOV_Close_cum_pct_change_from_10': 19, 'CDL_SEPARATINGLINES': 18, 'MCGD_10': 18, 'VIDYA_14': 17, 'USDBRL_Close_cum_pct_change_from_10': 17, 'QS_10': 16, 'TRUERANGE_1': 16, 'CDL_ONNECK': 15, 'CDL_UNIQUE3RIVER': 15, 'GOLD_Close_cum_pct_change_from_10': 15, 'CDL_STALLEDPATTERN': 12, 'CMF_20': 12, 'KCLe_20_2': 12, 'THERMO_20_2_0.5': 12, 'CDL_INNECK': 11, 'CDL_LADDERBOTTOM': 11, 'CDL_MORNINGDOJISTAR': 11, 'PGO_14': 11, 'Dividends': 9, 'CDL_EVENINGDOJISTAR': 9, 'GOLD_Close_cum_pct_change_from_1_month': 9, 'UO_7_14_28': 8, 'BCOMKW': 8, 'GOLD_Close_cum_pct_change_from_2_month': 8, 'CDL_COUNTERATTACK': 7, 'SKEW_30': 7, 'CDL_HIKKAKEMOD': 6, 'CDL_STICKSANDWICH': 6, 'PDIST': 6, 'VHF_28': 6, 'CDL_2CROWS': 5, 'CDL_3LINESTRIKE': 5, 'BCOMGO': 5, 'BCOMHO': 5, 'BCOMSI': 5, 'BCOMZS': 5, 'CDL_3WHITESOLDIERS': 4, 'CHOP_14_1_100': 4, 'SMIo_5_20_5': 4, 'BCOMPE': 4, 'BCOMNG': 4, 'BCOMSN': 4, 'CDL_3BLACKCROWS': 3, 'BCOMCL': 3, 'Stock Splits': 2, 'Open_pct_change': 2, 'High_pct_change': 2, 'Open_pct_change_lag_f1': 2, 'High_pct_change_lag_f1': 2, 'Open_pct_change_lag_f2': 2, 'High_pct_change_lag_f2': 2, 'Open_pct_change_lag_f3': 2, 'High_pct_change_lag_f3': 2, 'Open_pct_change_lag_f5': 2, 'High_pct_change_lag_f5': 2, 'Open_pct_change_lag_f10': 2, 'High_pct_change_lag_f10': 2, 'Open_pct_change_lag_f15': 2, 'High_pct_change_lag_f15': 2, 'Open_pct_change_lag_f20': 2, 'High_pct_change_lag_f20': 2, 'CCI_14_0.015': 2, 'AR_26': 1, 'CDL_ABANDONEDBABY': 1, 'CDL_UPSIDEGAP2CROWS': 1, 'RVGI_14_4': 1, 'VTXM_14': 1, 'BCOMBO': 1}.\n"
     ]
    }
   ],
   "source": [
    "# add the features to df_ticker\n",
    "df_w_features_all = feature_engineering(df)\n",
    "df_w_features_all = df_w_features_all.merge(df_merge, how='left', left_index=True, right_index=True)#.reset_index()\n",
    "df_w_features = df_w_features_all.loc['2001-01-31':'2022-12-27'].copy()\n",
    "if df_w_features.isna().sum().sum() > 0:\n",
    "    print(f'There are still {df_w_features.isna().sum().sum() > 0} missing values in the dataframe')\n",
    "    df_w_features.isna().sum(axis=1).sort_values(ascending=False)\n",
    "    df_w_features.isna().sum(axis=0).sort_values(ascending=False)\n",
    "df_w_features.shape\n",
    "\n",
    "# make target variable\n",
    "df_w_features['target'] = df_w_features['Close'].pct_change()\n",
    "\n",
    "# define macros for the columns\n",
    "COL_TO_CLIP = [str(col) for col in df_w_features.columns if col not in ['Date', 'target', 'Close_pct_change']]\n",
    "MOVE_FORWARD = [str(col) for col in df_w_features.columns if col not in ['Date', 'target']]\n",
    "FEATURES = [str(col) for col in df_w_features.columns if col not in ['Date', 'target']]\n",
    "TARGET = 'target'\n",
    "\n",
    "# shift the features one day forward so that each row has the features of the previous day and the target of the current day\n",
    "df_w_features.loc[:, MOVE_FORWARD] = df_w_features.loc[:, MOVE_FORWARD].shift(1)\n",
    "# with open('columns_droped.txt', 'a') as f:\n",
    "#     f.write(str([str(col) for col in df_w_features.columns]))\n",
    "df_w_features = df_w_features.dropna().reset_index()\n",
    "\n",
    "# apply the fill_infinities function to the train and test data\n",
    "df_w_features, n_infinities = fill_infinities(df_w_features)\n",
    "print(f\"{n_infinities} infinities filled in the data.\")\n",
    "\n",
    "# apply the fill_outliers function to the train and test data for the columns after the 'Close_pct_change_lag_b1' column (pandas_ta columns)\n",
    "df_w_features.loc[:, COL_TO_CLIP], n_outliers = clip_outliers(df_w_features.loc[:, COL_TO_CLIP])\n",
    "print(f\"Outliers cliped by column: {n_outliers}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time series split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# import bayesian optimization libraries\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_objective, plot_histogram\n",
    "\n",
    "# import preprocessing libs\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X = df_w_features[FEATURES]\n",
    "y = df_w_features[TARGET]\n",
    "\n",
    "# DataFrame to store the results\n",
    "df_results = pd.DataFrame()\n",
    "df_results['y_test'] = y[-252:]\n",
    "\n",
    "# DataFrame to store the feature importances of each model\n",
    "df_feature_importances = pd.DataFrame(index=FEATURES)\n",
    "\n",
    "# Save model\n",
    "def save_objet(obj, filename):\n",
    "    import joblib\n",
    "    path_lst = joblib.dump(obj, filename)\n",
    "    for i in range(len(path_lst)):\n",
    "        print(f\"Object saved in {path_lst[i]}\")\n",
    "\n",
    "# Plot the hyperparameter tuning results\n",
    "def tuning_plots(bayes_search, param_dist):\n",
    "    for i in range(len(bayes_search.optimizer_results_)):\n",
    "        if len(bayes_search.optimizer_results_[i].models) == 0:\n",
    "            continue\n",
    "        # Plot the objective function\n",
    "        plot_objective(bayes_search.optimizer_results_[i])\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the hyperparameter chosen values\n",
    "        fig, axes = plt.subplots(1, len(param_dist[i]), figsize=(15, 5))\n",
    "        for j, ax in enumerate(axes):\n",
    "            plot_histogram(bayes_search.optimizer_results_[i], j, ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "def print_scores(bayes_search, test_index):\n",
    "    # Print the best parameters and the best score\n",
    "    print(\"Best parameters: \", bayes_search.best_params_)\n",
    "    print(\"Best Accuracy: {:.2f} %\".format(bayes_search.best_score_*100))\n",
    "\n",
    "    # get the r2 score of the best model\n",
    "    from sklearn.metrics import r2_score\n",
    "    best_model = bayes_search.best_estimator_\n",
    "    print(\"Best Model R2 score: {:.4f}\".format(\n",
    "                                        r2_score(\n",
    "                                        y.loc[list(test_index)],\n",
    "                                        best_model.predict(X.loc[list(test_index)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  OrderedDict([('pca__n_components', 0.3657244526385898), ('regressor__fit_intercept', True), ('regressor__positive', True)])\n",
      "Best Accuracy: -0.05 %\n",
      "Best Model R2 score: -0.0024\n",
      "Object saved in saved_models/fitted_pipe_mlr.joblib\n"
     ]
    }
   ],
   "source": [
    "# Multiple Linear Regression\n",
    "def multiple_linear_regression(X, y, df_results, df_feature_importances, verbose=False):\n",
    "\n",
    "    # import regression model\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=5, test_size=252*1, gap=1)\n",
    "\n",
    "    #make pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('regressor', LinearRegression())\n",
    "        ])\n",
    "\n",
    "    # Define the hyperparameters to be tuned using BayesSearchCV's search space object\n",
    "    param_dist = [{\n",
    "        'pca__n_components': Real(0.3, 0.8),\n",
    "        'regressor__fit_intercept': Categorical([True, False]),\n",
    "        'regressor__positive': Categorical([True, False])\n",
    "    }]\n",
    "\n",
    "    # Create the BayesSearchCV object\n",
    "    bayes_search = BayesSearchCV(pipe, param_dist, n_iter=25, scoring='r2', cv=tss, n_jobs=-1)\n",
    "\n",
    "    # Fit the BayesSearchCV object to the data\n",
    "    bayes_search.fit(X[:-252], y[:-252])\n",
    "\n",
    "    # get the last train/test split indexes\n",
    "    for _, test_index in tss.split(X):\n",
    "        pass\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    df_results['mlr_y_pred'] = bayes_search.best_estimator_.predict(X.loc[list(test_index)])\n",
    "\n",
    "    print_scores(bayes_search, test_index)\n",
    "    if verbose:\n",
    "        tuning_plots(bayes_search, param_dist)\n",
    "        display(pd.DataFrame(bayes_search.cv_results_))\n",
    "    save_objet(bayes_search, 'saved_models/fitted_pipe_mlr.joblib')\n",
    "    # df_feature_importances['mlr'] = bayes_search.best_estimator_.named_steps['regressor'].coef_\n",
    "\n",
    "    return df_results\n",
    "\n",
    "df_results = multiple_linear_regression(X, y, df_results, df_feature_importances)\n",
    "\n",
    "# first training results\n",
    "# Best parameters:  {'fit_intercept': False, 'positive': True}\n",
    "# Best Accuracy: -58.67 %\n",
    "# Best Model R2 score: -0.0234\n",
    "# Best parameters:  {'pca__n_components': 44, 'regressor__fit_intercept': True, 'regressor__positive': True}\n",
    "# Best Accuracy: -1.77 %\n",
    "# Best Model R2 score: -0.0169\n",
    "# Best parameters:  {'pca__n_components': 0.76, 'regressor__fit_intercept': True, 'regressor__positive': True}\n",
    "# Best Accuracy: -2.84 %\n",
    "# Best Model R2 score: -0.0111\n",
    "## with commodities, exchange rates, and bench (ibov)\n",
    "# Best parameters:  OrderedDict([('pca__n_components', 0.4269376887065359), ('regressor__fit_intercept', True), ('regressor__positive', True)])\n",
    "# Best Accuracy: -0.09 %\n",
    "# Best Model R2 score: -0.0021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  OrderedDict([('feature_selection__estimator__max_depth', 1), ('feature_selection__estimator__min_samples_leaf', 25), ('feature_selection__threshold', '3*mean'), ('regression__max_depth', 1), ('regression__min_samples_leaf', 13)])\n",
      "Best Accuracy: 0.24 %\n",
      "Best Model R2 score: -0.0940\n",
      "Object saved in saved_models/fitted_pipe_dtr.joblib\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression\n",
    "def deciosion_tree_regression(X, y, df_results, df_feature_importances, verbose=False):\n",
    "\n",
    "    # import regression model\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=5, test_size=252*1, gap=1)\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dim_reduction', PCA()),\n",
    "        ('feature_selection', SelectFromModel(DecisionTreeRegressor())),\n",
    "        ('regression', DecisionTreeRegressor(random_state = 0))\n",
    "    ])\n",
    "\n",
    "    # Define the hyperparameters to be tuned\n",
    "    param_dist = [\n",
    "        {\n",
    "        'dim_reduction__n_components': Real(0.3, 0.8),\n",
    "        'regression__max_depth': Integer(1, 5),\n",
    "        'regression__min_samples_leaf': Integer(1, 35)\n",
    "        },\n",
    "        {\n",
    "        'feature_selection__estimator__max_depth': Integer(1, 5),\n",
    "        'feature_selection__estimator__min_samples_leaf': Integer(1, 25),\n",
    "        'feature_selection__threshold': Categorical(['0.2*mean', '1*mean', '2*mean', '3*mean']),\n",
    "        'regression__max_depth': Integer(1, 5),\n",
    "        'regression__min_samples_leaf': Integer(1, 35)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create the BayesSearchCV object\n",
    "    bayes_search = BayesSearchCV(pipe, param_dist, n_iter=25, scoring='r2', cv=tss, n_jobs=-1)\n",
    "\n",
    "    # Fit the BayesSearchCV object to the data\n",
    "    bayes_search.fit(X[:-252], y[:-252])\n",
    "\n",
    "    # get the last train/test split indexes\n",
    "    for _, test_index in tss.split(X):\n",
    "        pass\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    df_results['dtr_y_pred'] = bayes_search.best_estimator_.predict(X.loc[list(test_index)])\n",
    "\n",
    "    print_scores(bayes_search, test_index)\n",
    "    if verbose:\n",
    "        tuning_plots(bayes_search, param_dist)\n",
    "        display(pd.DataFrame(bayes_search.cv_results_))\n",
    "    save_objet(bayes_search, 'saved_models/fitted_pipe_dtr.joblib')\n",
    "    # print(bayes_search.best_estimator_.named_steps['regression'].feature_importances_)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "df_results = deciosion_tree_regression(X, y, df_results, df_feature_importances)\n",
    "\n",
    "# first training results\n",
    "## with commodities, exchange rates, and bench (ibov)\n",
    "# Best parameters:  OrderedDict([('dim_reduction__n_components', 0.47854272610088305), ('regression__max_depth', 1), ('regression__min_samples_leaf', 25)])\n",
    "# Best Accuracy: 0.21 %\n",
    "# Best Model R2 score: 0.0064\n",
    "# Model saved in saved_models/fitted_pipe_dtr.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  OrderedDict([('dim_reduction__n_components', 0.8), ('regression__max_depth', 1), ('regression__min_samples_leaf', 18)])\n",
      "Best Accuracy: 0.34 %\n",
      "Best Model R2 score: -0.0043\n",
      "Object saved in saved_models/fitted_pipe_rfr.joblib\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regression\n",
    "def random_forest_regression(X, y, df_results, df_feature_importances, verbose=False):\n",
    "\n",
    "    # import regression model\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=5, test_size=252*1, gap=1)\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dim_reduction', PCA()),\n",
    "        ('feature_selection', SelectFromModel(RandomForestRegressor(n_estimators=10, random_state=0))),\n",
    "        ('regression', RandomForestRegressor(n_estimators=25, random_state=0))\n",
    "    ])\n",
    "\n",
    "    # Define the hyperparameters to be tuned\n",
    "    param_dist = [{\n",
    "        'dim_reduction__n_components': Real(0.3, 0.9),\n",
    "        'regression__max_depth': Integer(1, 3),\n",
    "        'regression__min_samples_leaf': Integer(5, 35)\n",
    "    },\n",
    "    {\n",
    "        'feature_selection__estimator__max_depth': Integer(1, 3),\n",
    "        'feature_selection__estimator__min_samples_leaf': Integer(5, 35),\n",
    "        'feature_selection__threshold': Categorical(['0.2*mean', '1*mean', '2*mean', '3*mean']),\n",
    "        'regression__max_depth': Integer(1, 3),\n",
    "        'regression__min_samples_leaf': Integer(5, 35)\n",
    "    }]\n",
    "\n",
    "    # Create the BayesSearchCV object\n",
    "    bayes_search = BayesSearchCV(pipe, param_dist, n_iter=25, scoring='r2', cv=tss, n_jobs=-1)\n",
    "\n",
    "    # Fit the BayesSearchCV object to the data\n",
    "    bayes_search.fit(X[:-252], y[:-252])\n",
    "\n",
    "    # get the last train/test split indexes\n",
    "    for _, test_index in tss.split(X):\n",
    "        pass\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    df_results['rfr_y_pred'] = bayes_search.best_estimator_.predict(X.loc[list(test_index)])\n",
    "\n",
    "    print_scores(bayes_search, test_index)\n",
    "    if verbose:\n",
    "        tuning_plots(bayes_search, param_dist)\n",
    "        display(pd.DataFrame(bayes_search.cv_results_))\n",
    "    save_objet(bayes_search, 'saved_models/fitted_pipe_rfr.joblib')\n",
    "\n",
    "    # store feature importance\n",
    "    # loop over the features used in the best model and add the feature importance to the dataframe\n",
    "    # for i, feature in enumerate(bayes_search.best_estimator_.named_steps['feature_selection'].get_support(indices=True)):\n",
    "    #     df_feature_importances.loc[df_feature_importances.index[feature], 'rfr'] = bayes_search.best_estimator_.named_steps['regression'].feature_importances_[i]\n",
    "    # df_feature_importances.fillna(0, inplace=True)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "df_results = random_forest_regression(X, y, df_results, df_feature_importances)\n",
    "\n",
    "# Best parameters:  {'dim_reduction__n_components': 0.74, 'regression__max_depth': 1, 'regression__max_features': 1.0, 'regression__min_samples_leaf': 29, 'regression__n_estimators': 10}\n",
    "# Best Accuracy: 0.04 %\n",
    "# Best Model R2 score: -0.0015\n",
    "## with commodities, exchange rates, and bench (ibov)\n",
    "# Best parameters:  OrderedDict([('dim_reduction__n_components', 0.8), ('regression__max_depth', 1), ('regression__min_samples_leaf', 19)])\n",
    "# Best Accuracy: 0.01 %\n",
    "# Best Model R2 score: 0.0009\n",
    "# Model saved in saved_models/fitted_pipe_rtr.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/vfranco-/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  OrderedDict([('regression', LinearSVR(C=1e-06, max_iter=3000)), ('regression__C', 1e-06)])\n",
      "Best Accuracy: -0.33 %\n",
      "Best Model R2 score: -0.0074\n",
      "Object saved in saved_models/fitted_pipe_svr.joblib\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regression (SVR)\n",
    "def suport_vector_regression(X, y, df_results, verbose=False):\n",
    "\n",
    "    from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=5, test_size=252*1, gap=1)\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dim_reduction', PCA()),\n",
    "        ('feature_selection', SelectFromModel(SVR(kernel='linear'))),\n",
    "        ('regression', SVR())\n",
    "    ])\n",
    "\n",
    "    # Define the hyperparameters to be tuned\n",
    "    param_dist = [\n",
    "        {\n",
    "        'regression': [LinearSVR(max_iter=3000)],\n",
    "        'regression__C': Real(1e-6, 1e+1, 'log-uniform'),\n",
    "        },\n",
    "        {\n",
    "        'dim_reduction__n_components': Real(0.3, 0.9),\n",
    "        'regression__kernel': Categorical(['linear']),\n",
    "        'regression__C': Real(1e-6, 1, 'log-uniform')\n",
    "        },\n",
    "        {\n",
    "        'feature_selection__threshold': Categorical(['0.2*mean', '1*mean', '2*mean', '3*mean']),\n",
    "        'feature_selection__estimator__kernel': Categorical(['linear']),\n",
    "        'feature_selection__estimator__C': Real(1e-6, 1, 'log-uniform'),\n",
    "        'regression__kernel': Categorical(['linear']),\n",
    "        'regression__C': Real(1e-6, 1, 'log-uniform')\n",
    "        },\n",
    "        {\n",
    "        'dim_reduction__n_components': Real(0.3, 0.9),\n",
    "        'regression__kernel': Categorical(['rbf', 'sigmoid']),\n",
    "        'regression__gamma': Real(1e-6, 1e+1, 'log-uniform'),\n",
    "        'regression__C': Real(1e-6, 1, 'log-uniform'),\n",
    "        'regression__coef0': Real(-1, 1)\n",
    "        }]\n",
    "\n",
    "    # Create the BayesSearchCV object\n",
    "    bayes_search = BayesSearchCV(pipe, param_dist, n_iter=25, scoring='r2', cv=tss, n_jobs=-1)\n",
    "\n",
    "    # Fit the BayesSearchCV object to the data\n",
    "    bayes_search.fit(X[:-252], y[:-252])\n",
    "\n",
    "    # get the last train/test split indexes\n",
    "    for _, test_index in tss.split(X):\n",
    "        pass\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    df_results['svr_y_pred'] = bayes_search.best_estimator_.predict(X.loc[list(test_index)])\n",
    "\n",
    "    print_scores(bayes_search, test_index)\n",
    "    if verbose:\n",
    "        tuning_plots(bayes_search, param_dist)\n",
    "        display(pd.DataFrame(bayes_search.cv_results_))\n",
    "    save_objet(bayes_search, 'saved_models/fitted_pipe_svr.joblib')\n",
    "\n",
    "    return df_results\n",
    "\n",
    "df_results = suport_vector_regression(X, y, df_results)\n",
    "\n",
    "# first training results\n",
    "# Best parameters:  {'dim_reduction__n_components': 0.8, 'regression__C': 1e-06, 'regression__kernel': 'linear'}\n",
    "# Best Accuracy: -0.71 %\n",
    "# Best Model R2 score: 0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  OrderedDict([('pca__n_components', 0.8), ('regression__booster', 'gbtree'), ('regression__learning_rate', 0.07568800319787194), ('regression__max_depth', 1)])\n",
      "Best Accuracy: -2.19 %\n",
      "Best Model R2 score: -0.0005\n",
      "Object saved in saved_models/fitted_pipe_dtr.joblib\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "def XGBoost(X, y, df_results, df_feature_importances, verbose=False):\n",
    "\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=5, test_size=252*1, gap=1)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('feature_selection', SelectFromModel(XGBRegressor(n_estimators=10, random_state=0))),\n",
    "        ('regression', XGBRegressor(random_state=0))\n",
    "    ])\n",
    "\n",
    "    # define the hyperparameters to be tuned\n",
    "    param_dist = [{\n",
    "        'pca__n_components': Real(0.3, 0.9),\n",
    "        'regression__booster': Categorical(['gbtree']),\n",
    "        'regression__max_depth': Integer(1, 5),\n",
    "        'regression__learning_rate': Real(1e-4, 3e-1, 'log-uniform')\n",
    "        }]\n",
    "\n",
    "    # Create the BayesSearchCV object\n",
    "    bayes_search = BayesSearchCV(pipe, param_dist, n_iter=25, scoring='r2', cv=tss, n_jobs=-1)\n",
    "\n",
    "    # Fit the BayesSearchCV object to the data\n",
    "    bayes_search.fit(X[:-252], y[:-252])\n",
    "\n",
    "    # get the last train/test split indexes\n",
    "    for _, test_index in tss.split(X):\n",
    "        pass\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    df_results['xgb_y_pred'] = bayes_search.best_estimator_.predict(X.loc[list(test_index)])\n",
    "\n",
    "    print_scores(bayes_search, test_index)\n",
    "    if verbose:\n",
    "        tuning_plots(bayes_search, param_dist)\n",
    "        display(pd.DataFrame(bayes_search.cv_results_))\n",
    "    save_objet(bayes_search, 'saved_models/fitted_pipe_dtr.joblib')\n",
    "\n",
    "    # store feature importance\n",
    "    # loop over the features used in the best model and add the feature importance to the dataframe\n",
    "    for i, feature in enumerate(bayes_search.best_estimator_.named_steps['feature_selection'].get_support(indices=True)):\n",
    "        df_feature_importances.loc[df_feature_importances.index[feature], 'xgb3'] = bayes_search.best_estimator_.named_steps['regression'].feature_importances_[i]\n",
    "    df_feature_importances.fillna(0, inplace=True)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "df_results = XGBoost(X, y, df_results, df_feature_importances)\n",
    "\n",
    "# Best parameters:  {'feature_selection__estimator__n_estimators': 10, 'feature_selection__threshold': '1*mean', 'regression__learning_rate': 0.3, 'regression__max_depth': 1, 'regression__n_estimators': 17}\n",
    "# Best Accuracy: -8.65 %\n",
    "# Best Model R2 score: -0.1026\n",
    "# Best parameters:  {'pca__n_components': 0.58, 'regression__learning_rate': 0.4, 'regression__max_depth': 1, 'regression__n_estimators': 10}\n",
    "# Best Accuracy: -36.28 %\n",
    "# Best Model R2 score: -0.1866\n",
    "## with commodities, exchange rates, and bench (ibov)\n",
    "# Best parameters:  OrderedDict([('pca__n_components', 0.8), ('regression__booster', 'gbtree'), ('regression__learning_rate', 0.07568800319787194), ('regression__max_depth', 1)])\n",
    "# Best Accuracy: -2.19 %\n",
    "# Best Model R2 score: -0.0005\n",
    "# Object saved in saved_models/fitted_pipe_dtr.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.053189\n",
      "0:\tlearn: 0.0270207\ttotal: 256ms\tremaining: 4m 16s\n",
      "1:\tlearn: 0.0269688\ttotal: 299ms\tremaining: 2m 29s\n",
      "2:\tlearn: 0.0269143\ttotal: 357ms\tremaining: 1m 58s\n",
      "3:\tlearn: 0.0268807\ttotal: 398ms\tremaining: 1m 39s\n",
      "4:\tlearn: 0.0268425\ttotal: 449ms\tremaining: 1m 29s\n",
      "5:\tlearn: 0.0268018\ttotal: 501ms\tremaining: 1m 22s\n",
      "6:\tlearn: 0.0267580\ttotal: 549ms\tremaining: 1m 17s\n",
      "7:\tlearn: 0.0267080\ttotal: 604ms\tremaining: 1m 14s\n",
      "8:\tlearn: 0.0266668\ttotal: 654ms\tremaining: 1m 11s\n",
      "9:\tlearn: 0.0266175\ttotal: 704ms\tremaining: 1m 9s\n",
      "10:\tlearn: 0.0265872\ttotal: 754ms\tremaining: 1m 7s\n",
      "11:\tlearn: 0.0265450\ttotal: 805ms\tremaining: 1m 6s\n",
      "12:\tlearn: 0.0265140\ttotal: 854ms\tremaining: 1m 4s\n",
      "13:\tlearn: 0.0264734\ttotal: 896ms\tremaining: 1m 3s\n",
      "14:\tlearn: 0.0264437\ttotal: 944ms\tremaining: 1m 2s\n",
      "15:\tlearn: 0.0264057\ttotal: 997ms\tremaining: 1m 1s\n",
      "16:\tlearn: 0.0263772\ttotal: 1.04s\tremaining: 1m\n",
      "17:\tlearn: 0.0263388\ttotal: 1.09s\tremaining: 59.3s\n",
      "18:\tlearn: 0.0263148\ttotal: 1.18s\tremaining: 1m\n",
      "19:\tlearn: 0.0262723\ttotal: 1.26s\tremaining: 1m 1s\n",
      "20:\tlearn: 0.0262375\ttotal: 1.32s\tremaining: 1m 1s\n",
      "21:\tlearn: 0.0262021\ttotal: 1.38s\tremaining: 1m 1s\n",
      "22:\tlearn: 0.0261615\ttotal: 1.44s\tremaining: 1m 1s\n",
      "23:\tlearn: 0.0261277\ttotal: 1.5s\tremaining: 1m\n",
      "24:\tlearn: 0.0260873\ttotal: 1.54s\tremaining: 1m\n",
      "25:\tlearn: 0.0260589\ttotal: 1.6s\tremaining: 60s\n",
      "26:\tlearn: 0.0260305\ttotal: 1.66s\tremaining: 59.7s\n",
      "27:\tlearn: 0.0259851\ttotal: 1.71s\tremaining: 59.4s\n",
      "28:\tlearn: 0.0259549\ttotal: 1.77s\tremaining: 59.4s\n",
      "29:\tlearn: 0.0259287\ttotal: 1.83s\tremaining: 59.3s\n",
      "30:\tlearn: 0.0258945\ttotal: 1.89s\tremaining: 59s\n",
      "31:\tlearn: 0.0258581\ttotal: 1.95s\tremaining: 58.9s\n",
      "32:\tlearn: 0.0258331\ttotal: 2s\tremaining: 58.5s\n",
      "33:\tlearn: 0.0257953\ttotal: 2.04s\tremaining: 57.9s\n",
      "34:\tlearn: 0.0257721\ttotal: 2.09s\tremaining: 57.7s\n",
      "35:\tlearn: 0.0257529\ttotal: 2.14s\tremaining: 57.2s\n",
      "36:\tlearn: 0.0257202\ttotal: 2.18s\tremaining: 56.9s\n",
      "37:\tlearn: 0.0257006\ttotal: 2.23s\tremaining: 56.5s\n",
      "38:\tlearn: 0.0256745\ttotal: 2.28s\tremaining: 56.3s\n",
      "39:\tlearn: 0.0256598\ttotal: 2.33s\tremaining: 55.9s\n",
      "40:\tlearn: 0.0256306\ttotal: 2.38s\tremaining: 55.7s\n",
      "41:\tlearn: 0.0256030\ttotal: 2.42s\tremaining: 55.2s\n",
      "42:\tlearn: 0.0255773\ttotal: 2.47s\tremaining: 54.9s\n",
      "43:\tlearn: 0.0255464\ttotal: 2.51s\tremaining: 54.6s\n",
      "44:\tlearn: 0.0255069\ttotal: 2.56s\tremaining: 54.2s\n",
      "45:\tlearn: 0.0254915\ttotal: 2.59s\tremaining: 53.8s\n",
      "46:\tlearn: 0.0254651\ttotal: 2.63s\tremaining: 53.3s\n",
      "47:\tlearn: 0.0254385\ttotal: 2.67s\tremaining: 52.9s\n",
      "48:\tlearn: 0.0254201\ttotal: 2.71s\tremaining: 52.7s\n",
      "49:\tlearn: 0.0253947\ttotal: 2.76s\tremaining: 52.5s\n",
      "50:\tlearn: 0.0253735\ttotal: 2.8s\tremaining: 52.1s\n",
      "51:\tlearn: 0.0253373\ttotal: 2.84s\tremaining: 51.8s\n",
      "52:\tlearn: 0.0253137\ttotal: 2.88s\tremaining: 51.5s\n",
      "53:\tlearn: 0.0252911\ttotal: 2.92s\tremaining: 51.2s\n",
      "54:\tlearn: 0.0252708\ttotal: 2.97s\tremaining: 51.1s\n",
      "55:\tlearn: 0.0252416\ttotal: 3.01s\tremaining: 50.8s\n",
      "56:\tlearn: 0.0252221\ttotal: 3.06s\tremaining: 50.6s\n",
      "57:\tlearn: 0.0251933\ttotal: 3.11s\tremaining: 50.5s\n",
      "58:\tlearn: 0.0251539\ttotal: 3.15s\tremaining: 50.3s\n",
      "59:\tlearn: 0.0251160\ttotal: 3.2s\tremaining: 50.1s\n",
      "60:\tlearn: 0.0250943\ttotal: 3.24s\tremaining: 49.8s\n",
      "61:\tlearn: 0.0250636\ttotal: 3.28s\tremaining: 49.6s\n",
      "62:\tlearn: 0.0250372\ttotal: 3.31s\tremaining: 49.3s\n",
      "63:\tlearn: 0.0250200\ttotal: 3.35s\tremaining: 49.1s\n",
      "64:\tlearn: 0.0249953\ttotal: 3.4s\tremaining: 48.9s\n",
      "65:\tlearn: 0.0249749\ttotal: 3.44s\tremaining: 48.7s\n",
      "66:\tlearn: 0.0249478\ttotal: 3.48s\tremaining: 48.5s\n",
      "67:\tlearn: 0.0249159\ttotal: 3.52s\tremaining: 48.2s\n",
      "68:\tlearn: 0.0248983\ttotal: 3.55s\tremaining: 47.9s\n",
      "69:\tlearn: 0.0248790\ttotal: 3.59s\tremaining: 47.7s\n",
      "70:\tlearn: 0.0248660\ttotal: 3.63s\tremaining: 47.5s\n",
      "71:\tlearn: 0.0248440\ttotal: 3.67s\tremaining: 47.3s\n",
      "72:\tlearn: 0.0248092\ttotal: 3.71s\tremaining: 47.1s\n",
      "73:\tlearn: 0.0247860\ttotal: 3.75s\tremaining: 47s\n",
      "74:\tlearn: 0.0247685\ttotal: 3.81s\tremaining: 46.9s\n",
      "75:\tlearn: 0.0247366\ttotal: 3.85s\tremaining: 46.8s\n",
      "76:\tlearn: 0.0247063\ttotal: 3.89s\tremaining: 46.6s\n",
      "77:\tlearn: 0.0246945\ttotal: 3.92s\tremaining: 46.4s\n",
      "78:\tlearn: 0.0246683\ttotal: 3.96s\tremaining: 46.1s\n",
      "79:\tlearn: 0.0246431\ttotal: 4s\tremaining: 46s\n",
      "80:\tlearn: 0.0246334\ttotal: 4.04s\tremaining: 45.8s\n",
      "81:\tlearn: 0.0246097\ttotal: 4.08s\tremaining: 45.7s\n",
      "82:\tlearn: 0.0245845\ttotal: 4.11s\tremaining: 45.4s\n",
      "83:\tlearn: 0.0245697\ttotal: 4.15s\tremaining: 45.2s\n",
      "84:\tlearn: 0.0245506\ttotal: 4.18s\tremaining: 45s\n",
      "85:\tlearn: 0.0245281\ttotal: 4.22s\tremaining: 44.8s\n",
      "86:\tlearn: 0.0245051\ttotal: 4.26s\tremaining: 44.7s\n",
      "87:\tlearn: 0.0244787\ttotal: 4.3s\tremaining: 44.6s\n",
      "88:\tlearn: 0.0244455\ttotal: 4.34s\tremaining: 44.4s\n",
      "89:\tlearn: 0.0244191\ttotal: 4.38s\tremaining: 44.2s\n",
      "90:\tlearn: 0.0244055\ttotal: 4.41s\tremaining: 44.1s\n",
      "91:\tlearn: 0.0243778\ttotal: 4.45s\tremaining: 43.9s\n",
      "92:\tlearn: 0.0243452\ttotal: 4.49s\tremaining: 43.7s\n",
      "93:\tlearn: 0.0243272\ttotal: 4.53s\tremaining: 43.6s\n",
      "94:\tlearn: 0.0242953\ttotal: 4.56s\tremaining: 43.4s\n",
      "95:\tlearn: 0.0242761\ttotal: 4.6s\tremaining: 43.3s\n",
      "96:\tlearn: 0.0242496\ttotal: 4.64s\tremaining: 43.2s\n",
      "97:\tlearn: 0.0242188\ttotal: 4.67s\tremaining: 43s\n",
      "98:\tlearn: 0.0241967\ttotal: 4.71s\tremaining: 42.9s\n",
      "99:\tlearn: 0.0241737\ttotal: 4.76s\tremaining: 42.8s\n",
      "100:\tlearn: 0.0241495\ttotal: 4.8s\tremaining: 42.7s\n",
      "101:\tlearn: 0.0241337\ttotal: 4.83s\tremaining: 42.5s\n",
      "102:\tlearn: 0.0241146\ttotal: 4.87s\tremaining: 42.4s\n",
      "103:\tlearn: 0.0240961\ttotal: 4.9s\tremaining: 42.3s\n",
      "104:\tlearn: 0.0240675\ttotal: 4.95s\tremaining: 42.2s\n",
      "105:\tlearn: 0.0240510\ttotal: 4.99s\tremaining: 42.1s\n",
      "106:\tlearn: 0.0240376\ttotal: 5.03s\tremaining: 41.9s\n",
      "107:\tlearn: 0.0240234\ttotal: 5.06s\tremaining: 41.8s\n",
      "108:\tlearn: 0.0240054\ttotal: 5.09s\tremaining: 41.6s\n",
      "109:\tlearn: 0.0239894\ttotal: 5.13s\tremaining: 41.5s\n",
      "110:\tlearn: 0.0239656\ttotal: 5.17s\tremaining: 41.5s\n",
      "111:\tlearn: 0.0239499\ttotal: 5.22s\tremaining: 41.4s\n",
      "112:\tlearn: 0.0239325\ttotal: 5.26s\tremaining: 41.3s\n",
      "113:\tlearn: 0.0239109\ttotal: 5.29s\tremaining: 41.1s\n",
      "114:\tlearn: 0.0238829\ttotal: 5.33s\tremaining: 41.1s\n",
      "115:\tlearn: 0.0238694\ttotal: 5.37s\tremaining: 40.9s\n",
      "116:\tlearn: 0.0238557\ttotal: 5.42s\tremaining: 40.9s\n",
      "117:\tlearn: 0.0238351\ttotal: 5.46s\tremaining: 40.8s\n",
      "118:\tlearn: 0.0238182\ttotal: 5.5s\tremaining: 40.7s\n",
      "119:\tlearn: 0.0237991\ttotal: 5.54s\tremaining: 40.6s\n",
      "120:\tlearn: 0.0237750\ttotal: 5.57s\tremaining: 40.5s\n",
      "121:\tlearn: 0.0237522\ttotal: 5.61s\tremaining: 40.4s\n",
      "122:\tlearn: 0.0237280\ttotal: 5.66s\tremaining: 40.4s\n",
      "123:\tlearn: 0.0237131\ttotal: 5.7s\tremaining: 40.2s\n",
      "124:\tlearn: 0.0236961\ttotal: 5.73s\tremaining: 40.1s\n",
      "125:\tlearn: 0.0236783\ttotal: 5.78s\tremaining: 40.1s\n",
      "126:\tlearn: 0.0236583\ttotal: 5.82s\tremaining: 40s\n",
      "127:\tlearn: 0.0236316\ttotal: 5.87s\tremaining: 40s\n",
      "128:\tlearn: 0.0235936\ttotal: 5.92s\tremaining: 39.9s\n",
      "129:\tlearn: 0.0235693\ttotal: 5.96s\tremaining: 39.9s\n",
      "130:\tlearn: 0.0235491\ttotal: 6s\tremaining: 39.8s\n",
      "131:\tlearn: 0.0235251\ttotal: 6.03s\tremaining: 39.7s\n",
      "132:\tlearn: 0.0235058\ttotal: 6.07s\tremaining: 39.6s\n",
      "133:\tlearn: 0.0234924\ttotal: 6.12s\tremaining: 39.5s\n",
      "134:\tlearn: 0.0234772\ttotal: 6.15s\tremaining: 39.4s\n",
      "135:\tlearn: 0.0234575\ttotal: 6.19s\tremaining: 39.3s\n",
      "136:\tlearn: 0.0234305\ttotal: 6.22s\tremaining: 39.2s\n",
      "137:\tlearn: 0.0234183\ttotal: 6.26s\tremaining: 39.1s\n",
      "138:\tlearn: 0.0233914\ttotal: 6.31s\tremaining: 39.1s\n",
      "139:\tlearn: 0.0233740\ttotal: 6.36s\tremaining: 39s\n",
      "140:\tlearn: 0.0233488\ttotal: 6.4s\tremaining: 39s\n",
      "141:\tlearn: 0.0233337\ttotal: 6.44s\tremaining: 38.9s\n",
      "142:\tlearn: 0.0233185\ttotal: 6.48s\tremaining: 38.8s\n",
      "143:\tlearn: 0.0233030\ttotal: 6.51s\tremaining: 38.7s\n",
      "144:\tlearn: 0.0232859\ttotal: 6.56s\tremaining: 38.7s\n",
      "145:\tlearn: 0.0232593\ttotal: 6.6s\tremaining: 38.6s\n",
      "146:\tlearn: 0.0232390\ttotal: 6.63s\tremaining: 38.5s\n",
      "147:\tlearn: 0.0232203\ttotal: 6.67s\tremaining: 38.4s\n",
      "148:\tlearn: 0.0232016\ttotal: 6.7s\tremaining: 38.3s\n",
      "149:\tlearn: 0.0231799\ttotal: 6.75s\tremaining: 38.2s\n",
      "150:\tlearn: 0.0231619\ttotal: 6.79s\tremaining: 38.2s\n",
      "151:\tlearn: 0.0231411\ttotal: 6.83s\tremaining: 38.1s\n",
      "152:\tlearn: 0.0231240\ttotal: 6.88s\tremaining: 38.1s\n",
      "153:\tlearn: 0.0231085\ttotal: 6.92s\tremaining: 38s\n",
      "154:\tlearn: 0.0230933\ttotal: 6.98s\tremaining: 38.1s\n",
      "155:\tlearn: 0.0230707\ttotal: 7.04s\tremaining: 38.1s\n",
      "156:\tlearn: 0.0230609\ttotal: 7.1s\tremaining: 38.1s\n",
      "157:\tlearn: 0.0230320\ttotal: 7.14s\tremaining: 38.1s\n",
      "158:\tlearn: 0.0230183\ttotal: 7.18s\tremaining: 38s\n",
      "159:\tlearn: 0.0229928\ttotal: 7.22s\tremaining: 37.9s\n",
      "160:\tlearn: 0.0229674\ttotal: 7.26s\tremaining: 37.9s\n",
      "161:\tlearn: 0.0229473\ttotal: 7.3s\tremaining: 37.8s\n",
      "162:\tlearn: 0.0229303\ttotal: 7.34s\tremaining: 37.7s\n",
      "163:\tlearn: 0.0229191\ttotal: 7.38s\tremaining: 37.6s\n",
      "164:\tlearn: 0.0228988\ttotal: 7.41s\tremaining: 37.5s\n",
      "165:\tlearn: 0.0228774\ttotal: 7.46s\tremaining: 37.5s\n",
      "166:\tlearn: 0.0228572\ttotal: 7.5s\tremaining: 37.4s\n",
      "167:\tlearn: 0.0228354\ttotal: 7.54s\tremaining: 37.4s\n",
      "168:\tlearn: 0.0228105\ttotal: 7.58s\tremaining: 37.3s\n",
      "169:\tlearn: 0.0227890\ttotal: 7.63s\tremaining: 37.2s\n",
      "170:\tlearn: 0.0227768\ttotal: 7.67s\tremaining: 37.2s\n",
      "171:\tlearn: 0.0227621\ttotal: 7.72s\tremaining: 37.2s\n",
      "172:\tlearn: 0.0227450\ttotal: 7.76s\tremaining: 37.1s\n",
      "173:\tlearn: 0.0227259\ttotal: 7.8s\tremaining: 37s\n",
      "174:\tlearn: 0.0227040\ttotal: 7.84s\tremaining: 37s\n",
      "175:\tlearn: 0.0226899\ttotal: 7.88s\tremaining: 36.9s\n",
      "176:\tlearn: 0.0226634\ttotal: 7.93s\tremaining: 36.9s\n",
      "177:\tlearn: 0.0226410\ttotal: 7.97s\tremaining: 36.8s\n",
      "178:\tlearn: 0.0226277\ttotal: 8.01s\tremaining: 36.8s\n",
      "179:\tlearn: 0.0226101\ttotal: 8.05s\tremaining: 36.7s\n",
      "180:\tlearn: 0.0225796\ttotal: 8.1s\tremaining: 36.6s\n",
      "181:\tlearn: 0.0225598\ttotal: 8.14s\tremaining: 36.6s\n",
      "182:\tlearn: 0.0225432\ttotal: 8.18s\tremaining: 36.5s\n",
      "183:\tlearn: 0.0225223\ttotal: 8.21s\tremaining: 36.4s\n",
      "184:\tlearn: 0.0225064\ttotal: 8.25s\tremaining: 36.3s\n",
      "185:\tlearn: 0.0224806\ttotal: 8.3s\tremaining: 36.3s\n",
      "186:\tlearn: 0.0224629\ttotal: 8.34s\tremaining: 36.3s\n",
      "187:\tlearn: 0.0224344\ttotal: 8.42s\tremaining: 36.4s\n",
      "188:\tlearn: 0.0224083\ttotal: 8.46s\tremaining: 36.3s\n",
      "189:\tlearn: 0.0223953\ttotal: 8.52s\tremaining: 36.3s\n",
      "190:\tlearn: 0.0223691\ttotal: 8.57s\tremaining: 36.3s\n",
      "191:\tlearn: 0.0223506\ttotal: 8.62s\tremaining: 36.3s\n",
      "192:\tlearn: 0.0223242\ttotal: 8.67s\tremaining: 36.2s\n",
      "193:\tlearn: 0.0223060\ttotal: 8.72s\tremaining: 36.2s\n",
      "194:\tlearn: 0.0222835\ttotal: 8.78s\tremaining: 36.2s\n",
      "195:\tlearn: 0.0222587\ttotal: 8.82s\tremaining: 36.2s\n",
      "196:\tlearn: 0.0222429\ttotal: 8.85s\tremaining: 36.1s\n",
      "197:\tlearn: 0.0222255\ttotal: 8.89s\tremaining: 36s\n",
      "198:\tlearn: 0.0222073\ttotal: 8.93s\tremaining: 35.9s\n",
      "199:\tlearn: 0.0221799\ttotal: 8.98s\tremaining: 35.9s\n",
      "200:\tlearn: 0.0221545\ttotal: 9.01s\tremaining: 35.8s\n",
      "201:\tlearn: 0.0221309\ttotal: 9.05s\tremaining: 35.8s\n",
      "202:\tlearn: 0.0221043\ttotal: 9.09s\tremaining: 35.7s\n",
      "203:\tlearn: 0.0220792\ttotal: 9.13s\tremaining: 35.6s\n",
      "204:\tlearn: 0.0220721\ttotal: 9.18s\tremaining: 35.6s\n",
      "205:\tlearn: 0.0220491\ttotal: 9.22s\tremaining: 35.5s\n",
      "206:\tlearn: 0.0220303\ttotal: 9.26s\tremaining: 35.5s\n",
      "207:\tlearn: 0.0220083\ttotal: 9.3s\tremaining: 35.4s\n",
      "208:\tlearn: 0.0219885\ttotal: 9.34s\tremaining: 35.3s\n",
      "209:\tlearn: 0.0219715\ttotal: 9.39s\tremaining: 35.3s\n",
      "210:\tlearn: 0.0219422\ttotal: 9.42s\tremaining: 35.2s\n",
      "211:\tlearn: 0.0219235\ttotal: 9.46s\tremaining: 35.2s\n",
      "212:\tlearn: 0.0219058\ttotal: 9.5s\tremaining: 35.1s\n",
      "213:\tlearn: 0.0218805\ttotal: 9.54s\tremaining: 35s\n",
      "214:\tlearn: 0.0218657\ttotal: 9.59s\tremaining: 35s\n",
      "215:\tlearn: 0.0218388\ttotal: 9.63s\tremaining: 34.9s\n",
      "216:\tlearn: 0.0218175\ttotal: 9.66s\tremaining: 34.9s\n",
      "217:\tlearn: 0.0217965\ttotal: 9.7s\tremaining: 34.8s\n",
      "218:\tlearn: 0.0217744\ttotal: 9.74s\tremaining: 34.7s\n",
      "219:\tlearn: 0.0217594\ttotal: 9.79s\tremaining: 34.7s\n",
      "220:\tlearn: 0.0217289\ttotal: 9.82s\tremaining: 34.6s\n",
      "221:\tlearn: 0.0217107\ttotal: 9.86s\tremaining: 34.6s\n",
      "222:\tlearn: 0.0216886\ttotal: 9.91s\tremaining: 34.5s\n",
      "223:\tlearn: 0.0216701\ttotal: 9.95s\tremaining: 34.5s\n",
      "224:\tlearn: 0.0216490\ttotal: 9.99s\tremaining: 34.4s\n",
      "225:\tlearn: 0.0216314\ttotal: 10.1s\tremaining: 34.4s\n",
      "226:\tlearn: 0.0216089\ttotal: 10.1s\tremaining: 34.4s\n",
      "227:\tlearn: 0.0215879\ttotal: 10.2s\tremaining: 34.4s\n",
      "228:\tlearn: 0.0215723\ttotal: 10.2s\tremaining: 34.4s\n",
      "229:\tlearn: 0.0215478\ttotal: 10.3s\tremaining: 34.4s\n",
      "230:\tlearn: 0.0215248\ttotal: 10.3s\tremaining: 34.3s\n",
      "231:\tlearn: 0.0215042\ttotal: 10.3s\tremaining: 34.2s\n",
      "232:\tlearn: 0.0214841\ttotal: 10.4s\tremaining: 34.2s\n",
      "233:\tlearn: 0.0214654\ttotal: 10.4s\tremaining: 34.2s\n",
      "234:\tlearn: 0.0214420\ttotal: 10.5s\tremaining: 34.1s\n",
      "235:\tlearn: 0.0214262\ttotal: 10.5s\tremaining: 34.1s\n",
      "236:\tlearn: 0.0214062\ttotal: 10.6s\tremaining: 34s\n",
      "237:\tlearn: 0.0213798\ttotal: 10.6s\tremaining: 34s\n",
      "238:\tlearn: 0.0213683\ttotal: 10.7s\tremaining: 33.9s\n",
      "239:\tlearn: 0.0213495\ttotal: 10.7s\tremaining: 33.9s\n",
      "240:\tlearn: 0.0213278\ttotal: 10.8s\tremaining: 33.9s\n",
      "241:\tlearn: 0.0213081\ttotal: 10.8s\tremaining: 33.9s\n",
      "242:\tlearn: 0.0212816\ttotal: 10.9s\tremaining: 33.9s\n",
      "243:\tlearn: 0.0212593\ttotal: 10.9s\tremaining: 33.9s\n",
      "244:\tlearn: 0.0212419\ttotal: 11s\tremaining: 33.9s\n",
      "245:\tlearn: 0.0212230\ttotal: 11.1s\tremaining: 33.9s\n",
      "246:\tlearn: 0.0212046\ttotal: 11.1s\tremaining: 33.9s\n",
      "247:\tlearn: 0.0211783\ttotal: 11.2s\tremaining: 33.9s\n",
      "248:\tlearn: 0.0211571\ttotal: 11.3s\tremaining: 33.9s\n",
      "249:\tlearn: 0.0211316\ttotal: 11.3s\tremaining: 33.9s\n",
      "250:\tlearn: 0.0211122\ttotal: 11.4s\tremaining: 33.9s\n",
      "251:\tlearn: 0.0210914\ttotal: 11.4s\tremaining: 33.9s\n",
      "252:\tlearn: 0.0210640\ttotal: 11.5s\tremaining: 33.9s\n",
      "253:\tlearn: 0.0210408\ttotal: 11.5s\tremaining: 33.9s\n",
      "254:\tlearn: 0.0210258\ttotal: 11.6s\tremaining: 33.9s\n",
      "255:\tlearn: 0.0210065\ttotal: 11.7s\tremaining: 33.9s\n",
      "256:\tlearn: 0.0209876\ttotal: 11.7s\tremaining: 33.9s\n",
      "257:\tlearn: 0.0209702\ttotal: 11.8s\tremaining: 33.8s\n",
      "258:\tlearn: 0.0209498\ttotal: 11.8s\tremaining: 33.8s\n",
      "259:\tlearn: 0.0209273\ttotal: 11.9s\tremaining: 33.7s\n",
      "260:\tlearn: 0.0209043\ttotal: 11.9s\tremaining: 33.7s\n",
      "261:\tlearn: 0.0208828\ttotal: 11.9s\tremaining: 33.7s\n",
      "262:\tlearn: 0.0208662\ttotal: 12s\tremaining: 33.7s\n",
      "263:\tlearn: 0.0208472\ttotal: 12.1s\tremaining: 33.8s\n",
      "264:\tlearn: 0.0208282\ttotal: 12.2s\tremaining: 33.8s\n",
      "265:\tlearn: 0.0208075\ttotal: 12.2s\tremaining: 33.8s\n",
      "266:\tlearn: 0.0207819\ttotal: 12.3s\tremaining: 33.8s\n",
      "267:\tlearn: 0.0207650\ttotal: 12.4s\tremaining: 33.8s\n",
      "268:\tlearn: 0.0207449\ttotal: 12.4s\tremaining: 33.8s\n",
      "269:\tlearn: 0.0207226\ttotal: 12.5s\tremaining: 33.8s\n",
      "270:\tlearn: 0.0206968\ttotal: 12.6s\tremaining: 33.9s\n",
      "271:\tlearn: 0.0206798\ttotal: 12.7s\tremaining: 33.9s\n",
      "272:\tlearn: 0.0206576\ttotal: 12.7s\tremaining: 33.8s\n",
      "273:\tlearn: 0.0206374\ttotal: 12.8s\tremaining: 33.8s\n",
      "274:\tlearn: 0.0206198\ttotal: 12.8s\tremaining: 33.7s\n",
      "275:\tlearn: 0.0206088\ttotal: 12.9s\tremaining: 33.7s\n",
      "276:\tlearn: 0.0205840\ttotal: 12.9s\tremaining: 33.7s\n",
      "277:\tlearn: 0.0205701\ttotal: 13s\tremaining: 33.7s\n",
      "278:\tlearn: 0.0205442\ttotal: 13s\tremaining: 33.7s\n",
      "279:\tlearn: 0.0205273\ttotal: 13.1s\tremaining: 33.6s\n",
      "280:\tlearn: 0.0205102\ttotal: 13.1s\tremaining: 33.6s\n",
      "281:\tlearn: 0.0204826\ttotal: 13.2s\tremaining: 33.5s\n",
      "282:\tlearn: 0.0204637\ttotal: 13.2s\tremaining: 33.5s\n",
      "283:\tlearn: 0.0204430\ttotal: 13.3s\tremaining: 33.5s\n",
      "284:\tlearn: 0.0204158\ttotal: 13.3s\tremaining: 33.4s\n",
      "285:\tlearn: 0.0203899\ttotal: 13.4s\tremaining: 33.4s\n",
      "286:\tlearn: 0.0203678\ttotal: 13.4s\tremaining: 33.3s\n",
      "287:\tlearn: 0.0203457\ttotal: 13.5s\tremaining: 33.3s\n",
      "288:\tlearn: 0.0203248\ttotal: 13.5s\tremaining: 33.2s\n",
      "289:\tlearn: 0.0203067\ttotal: 13.5s\tremaining: 33.1s\n",
      "290:\tlearn: 0.0202869\ttotal: 13.6s\tremaining: 33.1s\n",
      "291:\tlearn: 0.0202716\ttotal: 13.6s\tremaining: 33s\n",
      "292:\tlearn: 0.0202472\ttotal: 13.7s\tremaining: 33s\n",
      "293:\tlearn: 0.0202326\ttotal: 13.7s\tremaining: 32.9s\n",
      "294:\tlearn: 0.0202086\ttotal: 13.7s\tremaining: 32.8s\n",
      "295:\tlearn: 0.0201896\ttotal: 13.8s\tremaining: 32.8s\n",
      "296:\tlearn: 0.0201701\ttotal: 13.8s\tremaining: 32.7s\n",
      "297:\tlearn: 0.0201410\ttotal: 13.9s\tremaining: 32.6s\n",
      "298:\tlearn: 0.0201287\ttotal: 13.9s\tremaining: 32.6s\n",
      "299:\tlearn: 0.0201087\ttotal: 13.9s\tremaining: 32.5s\n",
      "300:\tlearn: 0.0200891\ttotal: 14s\tremaining: 32.4s\n",
      "301:\tlearn: 0.0200696\ttotal: 14s\tremaining: 32.4s\n",
      "302:\tlearn: 0.0200563\ttotal: 14s\tremaining: 32.3s\n",
      "303:\tlearn: 0.0200350\ttotal: 14.1s\tremaining: 32.2s\n",
      "304:\tlearn: 0.0200190\ttotal: 14.1s\tremaining: 32.2s\n",
      "305:\tlearn: 0.0200003\ttotal: 14.2s\tremaining: 32.1s\n",
      "306:\tlearn: 0.0199911\ttotal: 14.2s\tremaining: 32.1s\n",
      "307:\tlearn: 0.0199672\ttotal: 14.3s\tremaining: 32.1s\n",
      "308:\tlearn: 0.0199506\ttotal: 14.3s\tremaining: 32s\n",
      "309:\tlearn: 0.0199349\ttotal: 14.4s\tremaining: 32s\n",
      "310:\tlearn: 0.0199086\ttotal: 14.4s\tremaining: 31.9s\n",
      "311:\tlearn: 0.0198923\ttotal: 14.4s\tremaining: 31.8s\n",
      "312:\tlearn: 0.0198725\ttotal: 14.5s\tremaining: 31.8s\n",
      "313:\tlearn: 0.0198571\ttotal: 14.5s\tremaining: 31.7s\n",
      "314:\tlearn: 0.0198364\ttotal: 14.5s\tremaining: 31.6s\n",
      "315:\tlearn: 0.0198162\ttotal: 14.6s\tremaining: 31.6s\n",
      "316:\tlearn: 0.0197966\ttotal: 14.6s\tremaining: 31.5s\n",
      "317:\tlearn: 0.0197890\ttotal: 14.7s\tremaining: 31.4s\n",
      "318:\tlearn: 0.0197743\ttotal: 14.7s\tremaining: 31.4s\n",
      "319:\tlearn: 0.0197606\ttotal: 14.7s\tremaining: 31.3s\n",
      "320:\tlearn: 0.0197438\ttotal: 14.8s\tremaining: 31.2s\n",
      "321:\tlearn: 0.0197260\ttotal: 14.8s\tremaining: 31.2s\n",
      "322:\tlearn: 0.0197045\ttotal: 14.8s\tremaining: 31.1s\n",
      "323:\tlearn: 0.0196859\ttotal: 14.9s\tremaining: 31s\n",
      "324:\tlearn: 0.0196696\ttotal: 14.9s\tremaining: 31s\n",
      "325:\tlearn: 0.0196533\ttotal: 14.9s\tremaining: 30.9s\n",
      "326:\tlearn: 0.0196338\ttotal: 15s\tremaining: 30.8s\n",
      "327:\tlearn: 0.0196123\ttotal: 15s\tremaining: 30.8s\n",
      "328:\tlearn: 0.0196005\ttotal: 15s\tremaining: 30.7s\n",
      "329:\tlearn: 0.0195918\ttotal: 15.1s\tremaining: 30.6s\n",
      "330:\tlearn: 0.0195689\ttotal: 15.1s\tremaining: 30.6s\n",
      "331:\tlearn: 0.0195486\ttotal: 15.2s\tremaining: 30.5s\n",
      "332:\tlearn: 0.0195330\ttotal: 15.2s\tremaining: 30.5s\n",
      "333:\tlearn: 0.0195092\ttotal: 15.2s\tremaining: 30.4s\n",
      "334:\tlearn: 0.0194826\ttotal: 15.3s\tremaining: 30.3s\n",
      "335:\tlearn: 0.0194653\ttotal: 15.3s\tremaining: 30.3s\n",
      "336:\tlearn: 0.0194545\ttotal: 15.4s\tremaining: 30.2s\n",
      "337:\tlearn: 0.0194447\ttotal: 15.4s\tremaining: 30.2s\n",
      "338:\tlearn: 0.0194274\ttotal: 15.4s\tremaining: 30.1s\n",
      "339:\tlearn: 0.0194124\ttotal: 15.5s\tremaining: 30s\n",
      "340:\tlearn: 0.0193922\ttotal: 15.5s\tremaining: 30s\n",
      "341:\tlearn: 0.0193797\ttotal: 15.6s\tremaining: 30s\n",
      "342:\tlearn: 0.0193664\ttotal: 15.6s\tremaining: 29.9s\n",
      "343:\tlearn: 0.0193501\ttotal: 15.7s\tremaining: 29.9s\n",
      "344:\tlearn: 0.0193356\ttotal: 15.7s\tremaining: 29.9s\n",
      "345:\tlearn: 0.0193203\ttotal: 15.8s\tremaining: 29.8s\n",
      "346:\tlearn: 0.0193021\ttotal: 15.8s\tremaining: 29.7s\n",
      "347:\tlearn: 0.0192807\ttotal: 15.9s\tremaining: 29.7s\n",
      "348:\tlearn: 0.0192660\ttotal: 15.9s\tremaining: 29.7s\n",
      "349:\tlearn: 0.0192456\ttotal: 15.9s\tremaining: 29.6s\n",
      "350:\tlearn: 0.0192285\ttotal: 16s\tremaining: 29.6s\n",
      "351:\tlearn: 0.0192129\ttotal: 16s\tremaining: 29.5s\n",
      "352:\tlearn: 0.0191997\ttotal: 16.1s\tremaining: 29.5s\n",
      "353:\tlearn: 0.0191825\ttotal: 16.1s\tremaining: 29.4s\n",
      "354:\tlearn: 0.0191699\ttotal: 16.2s\tremaining: 29.4s\n",
      "355:\tlearn: 0.0191544\ttotal: 16.2s\tremaining: 29.4s\n",
      "356:\tlearn: 0.0191327\ttotal: 16.3s\tremaining: 29.3s\n",
      "357:\tlearn: 0.0191195\ttotal: 16.3s\tremaining: 29.3s\n",
      "358:\tlearn: 0.0190941\ttotal: 16.4s\tremaining: 29.2s\n",
      "359:\tlearn: 0.0190685\ttotal: 16.4s\tremaining: 29.2s\n",
      "360:\tlearn: 0.0190513\ttotal: 16.5s\tremaining: 29.2s\n",
      "361:\tlearn: 0.0190300\ttotal: 16.5s\tremaining: 29.1s\n",
      "362:\tlearn: 0.0190158\ttotal: 16.7s\tremaining: 29.3s\n",
      "363:\tlearn: 0.0189972\ttotal: 16.8s\tremaining: 29.3s\n",
      "364:\tlearn: 0.0189869\ttotal: 16.8s\tremaining: 29.3s\n",
      "365:\tlearn: 0.0189616\ttotal: 16.9s\tremaining: 29.2s\n",
      "366:\tlearn: 0.0189529\ttotal: 16.9s\tremaining: 29.2s\n",
      "367:\tlearn: 0.0189357\ttotal: 17s\tremaining: 29.1s\n",
      "368:\tlearn: 0.0189256\ttotal: 17s\tremaining: 29.1s\n",
      "369:\tlearn: 0.0189057\ttotal: 17.1s\tremaining: 29.1s\n",
      "370:\tlearn: 0.0188810\ttotal: 17.1s\tremaining: 29.1s\n",
      "371:\tlearn: 0.0188649\ttotal: 17.2s\tremaining: 29s\n",
      "372:\tlearn: 0.0188498\ttotal: 17.3s\tremaining: 29s\n",
      "373:\tlearn: 0.0188360\ttotal: 17.3s\tremaining: 29s\n",
      "374:\tlearn: 0.0188245\ttotal: 17.4s\tremaining: 29s\n",
      "375:\tlearn: 0.0188102\ttotal: 17.4s\tremaining: 28.9s\n",
      "376:\tlearn: 0.0187943\ttotal: 17.5s\tremaining: 28.9s\n",
      "377:\tlearn: 0.0187742\ttotal: 17.6s\tremaining: 28.9s\n",
      "378:\tlearn: 0.0187594\ttotal: 17.6s\tremaining: 28.9s\n",
      "379:\tlearn: 0.0187328\ttotal: 17.7s\tremaining: 28.8s\n",
      "380:\tlearn: 0.0187098\ttotal: 17.7s\tremaining: 28.8s\n",
      "381:\tlearn: 0.0186905\ttotal: 17.8s\tremaining: 28.8s\n",
      "382:\tlearn: 0.0186757\ttotal: 17.9s\tremaining: 28.8s\n",
      "383:\tlearn: 0.0186565\ttotal: 17.9s\tremaining: 28.7s\n",
      "384:\tlearn: 0.0186372\ttotal: 18s\tremaining: 28.7s\n",
      "385:\tlearn: 0.0186089\ttotal: 18s\tremaining: 28.6s\n",
      "386:\tlearn: 0.0185882\ttotal: 18.1s\tremaining: 28.6s\n",
      "387:\tlearn: 0.0185690\ttotal: 18.1s\tremaining: 28.6s\n",
      "388:\tlearn: 0.0185528\ttotal: 18.2s\tremaining: 28.6s\n",
      "389:\tlearn: 0.0185344\ttotal: 18.2s\tremaining: 28.5s\n",
      "390:\tlearn: 0.0185136\ttotal: 18.3s\tremaining: 28.5s\n",
      "391:\tlearn: 0.0184946\ttotal: 18.3s\tremaining: 28.4s\n",
      "392:\tlearn: 0.0184808\ttotal: 18.4s\tremaining: 28.4s\n",
      "393:\tlearn: 0.0184615\ttotal: 18.4s\tremaining: 28.3s\n",
      "394:\tlearn: 0.0184457\ttotal: 18.5s\tremaining: 28.3s\n",
      "395:\tlearn: 0.0184291\ttotal: 18.5s\tremaining: 28.2s\n",
      "396:\tlearn: 0.0184093\ttotal: 18.5s\tremaining: 28.2s\n",
      "397:\tlearn: 0.0183964\ttotal: 18.6s\tremaining: 28.1s\n",
      "398:\tlearn: 0.0183811\ttotal: 18.6s\tremaining: 28.1s\n",
      "399:\tlearn: 0.0183634\ttotal: 18.7s\tremaining: 28s\n",
      "400:\tlearn: 0.0183451\ttotal: 18.7s\tremaining: 28s\n",
      "401:\tlearn: 0.0183330\ttotal: 18.8s\tremaining: 28s\n",
      "402:\tlearn: 0.0183177\ttotal: 18.8s\tremaining: 27.9s\n",
      "403:\tlearn: 0.0183064\ttotal: 18.9s\tremaining: 27.9s\n",
      "404:\tlearn: 0.0182956\ttotal: 18.9s\tremaining: 27.8s\n",
      "405:\tlearn: 0.0182736\ttotal: 19s\tremaining: 27.8s\n",
      "406:\tlearn: 0.0182559\ttotal: 19.1s\tremaining: 27.8s\n",
      "407:\tlearn: 0.0182410\ttotal: 19.1s\tremaining: 27.8s\n",
      "408:\tlearn: 0.0182236\ttotal: 19.2s\tremaining: 27.7s\n",
      "409:\tlearn: 0.0182123\ttotal: 19.2s\tremaining: 27.7s\n",
      "410:\tlearn: 0.0181983\ttotal: 19.3s\tremaining: 27.7s\n",
      "411:\tlearn: 0.0181829\ttotal: 19.4s\tremaining: 27.6s\n",
      "412:\tlearn: 0.0181659\ttotal: 19.4s\tremaining: 27.6s\n",
      "413:\tlearn: 0.0181494\ttotal: 19.5s\tremaining: 27.5s\n",
      "414:\tlearn: 0.0181406\ttotal: 19.5s\tremaining: 27.5s\n",
      "415:\tlearn: 0.0181229\ttotal: 19.6s\tremaining: 27.5s\n",
      "416:\tlearn: 0.0181110\ttotal: 19.6s\tremaining: 27.4s\n",
      "417:\tlearn: 0.0180924\ttotal: 19.6s\tremaining: 27.3s\n",
      "418:\tlearn: 0.0180795\ttotal: 19.7s\tremaining: 27.3s\n",
      "419:\tlearn: 0.0180616\ttotal: 19.8s\tremaining: 27.3s\n",
      "420:\tlearn: 0.0180487\ttotal: 19.9s\tremaining: 27.3s\n",
      "421:\tlearn: 0.0180324\ttotal: 19.9s\tremaining: 27.3s\n",
      "422:\tlearn: 0.0180177\ttotal: 20s\tremaining: 27.3s\n",
      "423:\tlearn: 0.0179905\ttotal: 20.1s\tremaining: 27.3s\n",
      "424:\tlearn: 0.0179718\ttotal: 20.1s\tremaining: 27.2s\n",
      "425:\tlearn: 0.0179462\ttotal: 20.2s\tremaining: 27.2s\n",
      "426:\tlearn: 0.0179299\ttotal: 20.2s\tremaining: 27.1s\n",
      "427:\tlearn: 0.0179123\ttotal: 20.3s\tremaining: 27.1s\n",
      "428:\tlearn: 0.0178958\ttotal: 20.3s\tremaining: 27s\n",
      "429:\tlearn: 0.0178844\ttotal: 20.3s\tremaining: 27s\n",
      "430:\tlearn: 0.0178706\ttotal: 20.4s\tremaining: 26.9s\n",
      "431:\tlearn: 0.0178497\ttotal: 20.5s\tremaining: 26.9s\n",
      "432:\tlearn: 0.0178361\ttotal: 20.5s\tremaining: 26.9s\n",
      "433:\tlearn: 0.0178190\ttotal: 20.6s\tremaining: 26.8s\n",
      "434:\tlearn: 0.0178092\ttotal: 20.6s\tremaining: 26.8s\n",
      "435:\tlearn: 0.0177904\ttotal: 20.7s\tremaining: 26.8s\n",
      "436:\tlearn: 0.0177708\ttotal: 20.8s\tremaining: 26.8s\n",
      "437:\tlearn: 0.0177532\ttotal: 20.8s\tremaining: 26.7s\n",
      "438:\tlearn: 0.0177411\ttotal: 20.9s\tremaining: 26.7s\n",
      "439:\tlearn: 0.0177292\ttotal: 21s\tremaining: 26.7s\n",
      "440:\tlearn: 0.0177137\ttotal: 21s\tremaining: 26.7s\n",
      "441:\tlearn: 0.0176995\ttotal: 21.1s\tremaining: 26.7s\n",
      "442:\tlearn: 0.0176831\ttotal: 21.2s\tremaining: 26.6s\n",
      "443:\tlearn: 0.0176705\ttotal: 21.3s\tremaining: 26.6s\n",
      "444:\tlearn: 0.0176565\ttotal: 21.4s\tremaining: 26.6s\n",
      "445:\tlearn: 0.0176465\ttotal: 21.4s\tremaining: 26.6s\n",
      "446:\tlearn: 0.0176372\ttotal: 21.5s\tremaining: 26.6s\n",
      "447:\tlearn: 0.0176227\ttotal: 21.6s\tremaining: 26.6s\n",
      "448:\tlearn: 0.0176050\ttotal: 21.7s\tremaining: 26.6s\n",
      "449:\tlearn: 0.0175831\ttotal: 21.7s\tremaining: 26.6s\n",
      "450:\tlearn: 0.0175729\ttotal: 21.8s\tremaining: 26.5s\n",
      "451:\tlearn: 0.0175598\ttotal: 21.8s\tremaining: 26.5s\n",
      "452:\tlearn: 0.0175403\ttotal: 21.9s\tremaining: 26.4s\n",
      "453:\tlearn: 0.0175262\ttotal: 21.9s\tremaining: 26.4s\n",
      "454:\tlearn: 0.0175068\ttotal: 22s\tremaining: 26.3s\n",
      "455:\tlearn: 0.0174918\ttotal: 22s\tremaining: 26.3s\n",
      "456:\tlearn: 0.0174786\ttotal: 22.1s\tremaining: 26.2s\n",
      "457:\tlearn: 0.0174614\ttotal: 22.1s\tremaining: 26.2s\n",
      "458:\tlearn: 0.0174465\ttotal: 22.2s\tremaining: 26.1s\n",
      "459:\tlearn: 0.0174300\ttotal: 22.2s\tremaining: 26.1s\n",
      "460:\tlearn: 0.0174231\ttotal: 22.2s\tremaining: 26s\n",
      "461:\tlearn: 0.0174082\ttotal: 22.3s\tremaining: 26s\n",
      "462:\tlearn: 0.0173919\ttotal: 22.3s\tremaining: 25.9s\n",
      "463:\tlearn: 0.0173752\ttotal: 22.4s\tremaining: 25.9s\n",
      "464:\tlearn: 0.0173581\ttotal: 22.4s\tremaining: 25.8s\n",
      "465:\tlearn: 0.0173414\ttotal: 22.5s\tremaining: 25.8s\n",
      "466:\tlearn: 0.0173269\ttotal: 22.5s\tremaining: 25.7s\n",
      "467:\tlearn: 0.0173073\ttotal: 22.6s\tremaining: 25.6s\n",
      "468:\tlearn: 0.0172976\ttotal: 22.6s\tremaining: 25.6s\n",
      "469:\tlearn: 0.0172819\ttotal: 22.6s\tremaining: 25.5s\n",
      "470:\tlearn: 0.0172635\ttotal: 22.7s\tremaining: 25.5s\n",
      "471:\tlearn: 0.0172515\ttotal: 22.7s\tremaining: 25.4s\n",
      "472:\tlearn: 0.0172359\ttotal: 22.8s\tremaining: 25.4s\n",
      "473:\tlearn: 0.0172252\ttotal: 22.8s\tremaining: 25.3s\n",
      "474:\tlearn: 0.0172134\ttotal: 22.8s\tremaining: 25.2s\n",
      "475:\tlearn: 0.0171951\ttotal: 22.9s\tremaining: 25.2s\n",
      "476:\tlearn: 0.0171789\ttotal: 22.9s\tremaining: 25.1s\n",
      "477:\tlearn: 0.0171624\ttotal: 23s\tremaining: 25.1s\n",
      "478:\tlearn: 0.0171488\ttotal: 23s\tremaining: 25s\n",
      "479:\tlearn: 0.0171351\ttotal: 23s\tremaining: 25s\n",
      "480:\tlearn: 0.0171160\ttotal: 23.1s\tremaining: 24.9s\n",
      "481:\tlearn: 0.0170976\ttotal: 23.1s\tremaining: 24.8s\n",
      "482:\tlearn: 0.0170868\ttotal: 23.2s\tremaining: 24.8s\n",
      "483:\tlearn: 0.0170687\ttotal: 23.2s\tremaining: 24.8s\n",
      "484:\tlearn: 0.0170590\ttotal: 23.3s\tremaining: 24.7s\n",
      "485:\tlearn: 0.0170355\ttotal: 23.3s\tremaining: 24.7s\n",
      "486:\tlearn: 0.0170220\ttotal: 23.4s\tremaining: 24.6s\n",
      "487:\tlearn: 0.0170065\ttotal: 23.4s\tremaining: 24.6s\n",
      "488:\tlearn: 0.0169896\ttotal: 23.5s\tremaining: 24.5s\n",
      "489:\tlearn: 0.0169759\ttotal: 23.5s\tremaining: 24.5s\n",
      "490:\tlearn: 0.0169614\ttotal: 23.5s\tremaining: 24.4s\n",
      "491:\tlearn: 0.0169474\ttotal: 23.6s\tremaining: 24.4s\n",
      "492:\tlearn: 0.0169331\ttotal: 23.6s\tremaining: 24.3s\n",
      "493:\tlearn: 0.0169166\ttotal: 23.7s\tremaining: 24.2s\n",
      "494:\tlearn: 0.0169007\ttotal: 23.7s\tremaining: 24.2s\n",
      "495:\tlearn: 0.0168834\ttotal: 23.8s\tremaining: 24.1s\n",
      "496:\tlearn: 0.0168719\ttotal: 23.8s\tremaining: 24.1s\n",
      "497:\tlearn: 0.0168557\ttotal: 23.8s\tremaining: 24s\n",
      "498:\tlearn: 0.0168452\ttotal: 23.9s\tremaining: 24s\n",
      "499:\tlearn: 0.0168376\ttotal: 23.9s\tremaining: 23.9s\n",
      "500:\tlearn: 0.0168240\ttotal: 24s\tremaining: 23.9s\n",
      "501:\tlearn: 0.0168104\ttotal: 24s\tremaining: 23.8s\n",
      "502:\tlearn: 0.0167973\ttotal: 24s\tremaining: 23.8s\n",
      "503:\tlearn: 0.0167860\ttotal: 24.1s\tremaining: 23.7s\n",
      "504:\tlearn: 0.0167685\ttotal: 24.1s\tremaining: 23.6s\n",
      "505:\tlearn: 0.0167478\ttotal: 24.2s\tremaining: 23.6s\n",
      "506:\tlearn: 0.0167342\ttotal: 24.2s\tremaining: 23.5s\n",
      "507:\tlearn: 0.0167217\ttotal: 24.2s\tremaining: 23.5s\n",
      "508:\tlearn: 0.0167074\ttotal: 24.3s\tremaining: 23.4s\n",
      "509:\tlearn: 0.0166951\ttotal: 24.3s\tremaining: 23.4s\n",
      "510:\tlearn: 0.0166733\ttotal: 24.4s\tremaining: 23.3s\n",
      "511:\tlearn: 0.0166508\ttotal: 24.4s\tremaining: 23.3s\n",
      "512:\tlearn: 0.0166358\ttotal: 24.4s\tremaining: 23.2s\n",
      "513:\tlearn: 0.0166161\ttotal: 24.5s\tremaining: 23.1s\n",
      "514:\tlearn: 0.0166004\ttotal: 24.5s\tremaining: 23.1s\n",
      "515:\tlearn: 0.0165894\ttotal: 24.5s\tremaining: 23s\n",
      "516:\tlearn: 0.0165747\ttotal: 24.6s\tremaining: 23s\n",
      "517:\tlearn: 0.0165621\ttotal: 24.6s\tremaining: 22.9s\n",
      "518:\tlearn: 0.0165507\ttotal: 24.7s\tremaining: 22.9s\n",
      "519:\tlearn: 0.0165289\ttotal: 24.7s\tremaining: 22.8s\n",
      "520:\tlearn: 0.0165171\ttotal: 24.7s\tremaining: 22.7s\n",
      "521:\tlearn: 0.0165062\ttotal: 24.8s\tremaining: 22.7s\n",
      "522:\tlearn: 0.0164871\ttotal: 24.8s\tremaining: 22.6s\n",
      "523:\tlearn: 0.0164850\ttotal: 24.9s\tremaining: 22.6s\n",
      "524:\tlearn: 0.0164746\ttotal: 24.9s\tremaining: 22.5s\n",
      "525:\tlearn: 0.0164579\ttotal: 24.9s\tremaining: 22.5s\n",
      "526:\tlearn: 0.0164455\ttotal: 25s\tremaining: 22.4s\n",
      "527:\tlearn: 0.0164322\ttotal: 25s\tremaining: 22.4s\n",
      "528:\tlearn: 0.0164174\ttotal: 25s\tremaining: 22.3s\n",
      "529:\tlearn: 0.0164025\ttotal: 25.1s\tremaining: 22.2s\n",
      "530:\tlearn: 0.0163857\ttotal: 25.1s\tremaining: 22.2s\n",
      "531:\tlearn: 0.0163722\ttotal: 25.2s\tremaining: 22.1s\n",
      "532:\tlearn: 0.0163537\ttotal: 25.2s\tremaining: 22.1s\n",
      "533:\tlearn: 0.0163387\ttotal: 25.3s\tremaining: 22s\n",
      "534:\tlearn: 0.0163283\ttotal: 25.3s\tremaining: 22s\n",
      "535:\tlearn: 0.0163118\ttotal: 25.3s\tremaining: 21.9s\n",
      "536:\tlearn: 0.0162966\ttotal: 25.4s\tremaining: 21.9s\n",
      "537:\tlearn: 0.0162833\ttotal: 25.4s\tremaining: 21.8s\n",
      "538:\tlearn: 0.0162713\ttotal: 25.5s\tremaining: 21.8s\n",
      "539:\tlearn: 0.0162530\ttotal: 25.5s\tremaining: 21.7s\n",
      "540:\tlearn: 0.0162376\ttotal: 25.5s\tremaining: 21.7s\n",
      "541:\tlearn: 0.0162248\ttotal: 25.6s\tremaining: 21.6s\n",
      "542:\tlearn: 0.0162140\ttotal: 25.6s\tremaining: 21.6s\n",
      "543:\tlearn: 0.0162011\ttotal: 25.6s\tremaining: 21.5s\n",
      "544:\tlearn: 0.0161777\ttotal: 25.7s\tremaining: 21.4s\n",
      "545:\tlearn: 0.0161593\ttotal: 25.7s\tremaining: 21.4s\n",
      "546:\tlearn: 0.0161408\ttotal: 25.8s\tremaining: 21.3s\n",
      "547:\tlearn: 0.0161240\ttotal: 25.8s\tremaining: 21.3s\n",
      "548:\tlearn: 0.0161076\ttotal: 25.9s\tremaining: 21.3s\n",
      "549:\tlearn: 0.0160947\ttotal: 25.9s\tremaining: 21.2s\n",
      "550:\tlearn: 0.0160775\ttotal: 26s\tremaining: 21.2s\n",
      "551:\tlearn: 0.0160626\ttotal: 26s\tremaining: 21.1s\n",
      "552:\tlearn: 0.0160502\ttotal: 26.1s\tremaining: 21.1s\n",
      "553:\tlearn: 0.0160360\ttotal: 26.1s\tremaining: 21s\n",
      "554:\tlearn: 0.0160233\ttotal: 26.1s\tremaining: 21s\n",
      "555:\tlearn: 0.0160103\ttotal: 26.2s\tremaining: 20.9s\n",
      "556:\tlearn: 0.0159962\ttotal: 26.2s\tremaining: 20.9s\n",
      "557:\tlearn: 0.0159842\ttotal: 26.3s\tremaining: 20.8s\n",
      "558:\tlearn: 0.0159764\ttotal: 26.3s\tremaining: 20.8s\n",
      "559:\tlearn: 0.0159595\ttotal: 26.4s\tremaining: 20.7s\n",
      "560:\tlearn: 0.0159440\ttotal: 26.4s\tremaining: 20.7s\n",
      "561:\tlearn: 0.0159361\ttotal: 26.5s\tremaining: 20.6s\n",
      "562:\tlearn: 0.0159221\ttotal: 26.5s\tremaining: 20.6s\n",
      "563:\tlearn: 0.0159042\ttotal: 26.5s\tremaining: 20.5s\n",
      "564:\tlearn: 0.0158888\ttotal: 26.6s\tremaining: 20.5s\n",
      "565:\tlearn: 0.0158650\ttotal: 26.6s\tremaining: 20.4s\n",
      "566:\tlearn: 0.0158516\ttotal: 26.7s\tremaining: 20.4s\n",
      "567:\tlearn: 0.0158369\ttotal: 26.7s\tremaining: 20.3s\n",
      "568:\tlearn: 0.0158215\ttotal: 26.7s\tremaining: 20.3s\n",
      "569:\tlearn: 0.0158000\ttotal: 26.8s\tremaining: 20.2s\n",
      "570:\tlearn: 0.0157886\ttotal: 26.8s\tremaining: 20.2s\n",
      "571:\tlearn: 0.0157687\ttotal: 26.9s\tremaining: 20.1s\n",
      "572:\tlearn: 0.0157539\ttotal: 26.9s\tremaining: 20.1s\n",
      "573:\tlearn: 0.0157449\ttotal: 27s\tremaining: 20s\n",
      "574:\tlearn: 0.0157326\ttotal: 27s\tremaining: 20s\n",
      "575:\tlearn: 0.0157216\ttotal: 27s\tremaining: 19.9s\n",
      "576:\tlearn: 0.0157097\ttotal: 27.1s\tremaining: 19.8s\n",
      "577:\tlearn: 0.0156961\ttotal: 27.1s\tremaining: 19.8s\n",
      "578:\tlearn: 0.0156749\ttotal: 27.1s\tremaining: 19.7s\n",
      "579:\tlearn: 0.0156556\ttotal: 27.2s\tremaining: 19.7s\n",
      "580:\tlearn: 0.0156429\ttotal: 27.2s\tremaining: 19.6s\n",
      "581:\tlearn: 0.0156287\ttotal: 27.3s\tremaining: 19.6s\n",
      "582:\tlearn: 0.0156132\ttotal: 27.3s\tremaining: 19.5s\n",
      "583:\tlearn: 0.0155961\ttotal: 27.3s\tremaining: 19.5s\n",
      "584:\tlearn: 0.0155875\ttotal: 27.4s\tremaining: 19.4s\n",
      "585:\tlearn: 0.0155766\ttotal: 27.4s\tremaining: 19.4s\n",
      "586:\tlearn: 0.0155662\ttotal: 27.5s\tremaining: 19.3s\n",
      "587:\tlearn: 0.0155447\ttotal: 27.6s\tremaining: 19.3s\n",
      "588:\tlearn: 0.0155310\ttotal: 27.6s\tremaining: 19.3s\n",
      "589:\tlearn: 0.0155208\ttotal: 27.6s\tremaining: 19.2s\n",
      "590:\tlearn: 0.0155122\ttotal: 27.7s\tremaining: 19.2s\n",
      "591:\tlearn: 0.0155007\ttotal: 27.7s\tremaining: 19.1s\n",
      "592:\tlearn: 0.0154886\ttotal: 27.8s\tremaining: 19.1s\n",
      "593:\tlearn: 0.0154718\ttotal: 27.8s\tremaining: 19s\n",
      "594:\tlearn: 0.0154611\ttotal: 27.9s\tremaining: 19s\n",
      "595:\tlearn: 0.0154467\ttotal: 28s\tremaining: 19s\n",
      "596:\tlearn: 0.0154357\ttotal: 28s\tremaining: 18.9s\n",
      "597:\tlearn: 0.0154237\ttotal: 28.1s\tremaining: 18.9s\n",
      "598:\tlearn: 0.0154177\ttotal: 28.2s\tremaining: 18.9s\n",
      "599:\tlearn: 0.0154026\ttotal: 28.2s\tremaining: 18.8s\n",
      "600:\tlearn: 0.0153917\ttotal: 28.3s\tremaining: 18.8s\n",
      "601:\tlearn: 0.0153742\ttotal: 28.3s\tremaining: 18.7s\n",
      "602:\tlearn: 0.0153628\ttotal: 28.4s\tremaining: 18.7s\n",
      "603:\tlearn: 0.0153478\ttotal: 28.4s\tremaining: 18.6s\n",
      "604:\tlearn: 0.0153372\ttotal: 28.5s\tremaining: 18.6s\n",
      "605:\tlearn: 0.0153185\ttotal: 28.5s\tremaining: 18.5s\n",
      "606:\tlearn: 0.0153063\ttotal: 28.6s\tremaining: 18.5s\n",
      "607:\tlearn: 0.0152908\ttotal: 28.6s\tremaining: 18.4s\n",
      "608:\tlearn: 0.0152730\ttotal: 28.6s\tremaining: 18.4s\n",
      "609:\tlearn: 0.0152619\ttotal: 28.7s\tremaining: 18.3s\n",
      "610:\tlearn: 0.0152509\ttotal: 28.7s\tremaining: 18.3s\n",
      "611:\tlearn: 0.0152491\ttotal: 28.8s\tremaining: 18.2s\n",
      "612:\tlearn: 0.0152331\ttotal: 28.8s\tremaining: 18.2s\n",
      "613:\tlearn: 0.0152239\ttotal: 28.9s\tremaining: 18.1s\n",
      "614:\tlearn: 0.0152219\ttotal: 28.9s\tremaining: 18.1s\n",
      "615:\tlearn: 0.0152086\ttotal: 28.9s\tremaining: 18s\n",
      "616:\tlearn: 0.0151950\ttotal: 29s\tremaining: 18s\n",
      "617:\tlearn: 0.0151851\ttotal: 29s\tremaining: 17.9s\n",
      "618:\tlearn: 0.0151747\ttotal: 29.1s\tremaining: 17.9s\n",
      "619:\tlearn: 0.0151662\ttotal: 29.2s\tremaining: 17.9s\n",
      "620:\tlearn: 0.0151452\ttotal: 29.3s\tremaining: 17.9s\n",
      "621:\tlearn: 0.0151354\ttotal: 29.3s\tremaining: 17.8s\n",
      "622:\tlearn: 0.0151231\ttotal: 29.4s\tremaining: 17.8s\n",
      "623:\tlearn: 0.0151116\ttotal: 29.4s\tremaining: 17.7s\n",
      "624:\tlearn: 0.0151052\ttotal: 29.5s\tremaining: 17.7s\n",
      "625:\tlearn: 0.0150887\ttotal: 29.5s\tremaining: 17.6s\n",
      "626:\tlearn: 0.0150708\ttotal: 29.6s\tremaining: 17.6s\n",
      "627:\tlearn: 0.0150580\ttotal: 29.6s\tremaining: 17.5s\n",
      "628:\tlearn: 0.0150442\ttotal: 29.7s\tremaining: 17.5s\n",
      "629:\tlearn: 0.0150338\ttotal: 29.7s\tremaining: 17.5s\n",
      "630:\tlearn: 0.0150186\ttotal: 29.8s\tremaining: 17.4s\n",
      "631:\tlearn: 0.0150035\ttotal: 29.9s\tremaining: 17.4s\n",
      "632:\tlearn: 0.0149929\ttotal: 29.9s\tremaining: 17.3s\n",
      "633:\tlearn: 0.0149854\ttotal: 30s\tremaining: 17.3s\n",
      "634:\tlearn: 0.0149826\ttotal: 30.1s\tremaining: 17.3s\n",
      "635:\tlearn: 0.0149707\ttotal: 30.2s\tremaining: 17.3s\n",
      "636:\tlearn: 0.0149531\ttotal: 30.2s\tremaining: 17.2s\n",
      "637:\tlearn: 0.0149479\ttotal: 30.3s\tremaining: 17.2s\n",
      "638:\tlearn: 0.0149379\ttotal: 30.4s\tremaining: 17.2s\n",
      "639:\tlearn: 0.0149240\ttotal: 30.4s\tremaining: 17.1s\n",
      "640:\tlearn: 0.0149090\ttotal: 30.5s\tremaining: 17.1s\n",
      "641:\tlearn: 0.0148985\ttotal: 30.6s\tremaining: 17s\n",
      "642:\tlearn: 0.0148876\ttotal: 30.6s\tremaining: 17s\n",
      "643:\tlearn: 0.0148746\ttotal: 30.7s\tremaining: 16.9s\n",
      "644:\tlearn: 0.0148657\ttotal: 30.7s\tremaining: 16.9s\n",
      "645:\tlearn: 0.0148538\ttotal: 30.7s\tremaining: 16.8s\n",
      "646:\tlearn: 0.0148434\ttotal: 30.8s\tremaining: 16.8s\n",
      "647:\tlearn: 0.0148281\ttotal: 30.8s\tremaining: 16.8s\n",
      "648:\tlearn: 0.0148090\ttotal: 30.9s\tremaining: 16.7s\n",
      "649:\tlearn: 0.0147966\ttotal: 31s\tremaining: 16.7s\n",
      "650:\tlearn: 0.0147845\ttotal: 31s\tremaining: 16.6s\n",
      "651:\tlearn: 0.0147696\ttotal: 31.1s\tremaining: 16.6s\n",
      "652:\tlearn: 0.0147596\ttotal: 31.1s\tremaining: 16.5s\n",
      "653:\tlearn: 0.0147403\ttotal: 31.2s\tremaining: 16.5s\n",
      "654:\tlearn: 0.0147300\ttotal: 31.3s\tremaining: 16.5s\n",
      "655:\tlearn: 0.0147190\ttotal: 31.3s\tremaining: 16.4s\n",
      "656:\tlearn: 0.0147080\ttotal: 31.4s\tremaining: 16.4s\n",
      "657:\tlearn: 0.0146964\ttotal: 31.4s\tremaining: 16.3s\n",
      "658:\tlearn: 0.0146853\ttotal: 31.5s\tremaining: 16.3s\n",
      "659:\tlearn: 0.0146746\ttotal: 31.5s\tremaining: 16.2s\n",
      "660:\tlearn: 0.0146576\ttotal: 31.6s\tremaining: 16.2s\n",
      "661:\tlearn: 0.0146392\ttotal: 31.7s\tremaining: 16.2s\n",
      "662:\tlearn: 0.0146237\ttotal: 31.7s\tremaining: 16.1s\n",
      "663:\tlearn: 0.0146096\ttotal: 31.8s\tremaining: 16.1s\n",
      "664:\tlearn: 0.0145981\ttotal: 31.8s\tremaining: 16s\n",
      "665:\tlearn: 0.0145851\ttotal: 31.9s\tremaining: 16s\n",
      "666:\tlearn: 0.0145705\ttotal: 31.9s\tremaining: 15.9s\n",
      "667:\tlearn: 0.0145628\ttotal: 31.9s\tremaining: 15.9s\n",
      "668:\tlearn: 0.0145438\ttotal: 32s\tremaining: 15.8s\n",
      "669:\tlearn: 0.0145319\ttotal: 32s\tremaining: 15.8s\n",
      "670:\tlearn: 0.0145254\ttotal: 32.1s\tremaining: 15.7s\n",
      "671:\tlearn: 0.0145127\ttotal: 32.2s\tremaining: 15.7s\n",
      "672:\tlearn: 0.0145045\ttotal: 32.2s\tremaining: 15.7s\n",
      "673:\tlearn: 0.0144920\ttotal: 32.3s\tremaining: 15.6s\n",
      "674:\tlearn: 0.0144818\ttotal: 32.3s\tremaining: 15.6s\n",
      "675:\tlearn: 0.0144681\ttotal: 32.4s\tremaining: 15.5s\n",
      "676:\tlearn: 0.0144561\ttotal: 32.4s\tremaining: 15.5s\n",
      "677:\tlearn: 0.0144402\ttotal: 32.5s\tremaining: 15.4s\n",
      "678:\tlearn: 0.0144319\ttotal: 32.5s\tremaining: 15.4s\n",
      "679:\tlearn: 0.0144178\ttotal: 32.6s\tremaining: 15.3s\n",
      "680:\tlearn: 0.0144055\ttotal: 32.7s\tremaining: 15.3s\n",
      "681:\tlearn: 0.0143907\ttotal: 32.7s\tremaining: 15.3s\n",
      "682:\tlearn: 0.0143766\ttotal: 32.8s\tremaining: 15.2s\n",
      "683:\tlearn: 0.0143750\ttotal: 32.9s\tremaining: 15.2s\n",
      "684:\tlearn: 0.0143627\ttotal: 32.9s\tremaining: 15.1s\n",
      "685:\tlearn: 0.0143531\ttotal: 33s\tremaining: 15.1s\n",
      "686:\tlearn: 0.0143444\ttotal: 33.1s\tremaining: 15.1s\n",
      "687:\tlearn: 0.0143400\ttotal: 33.1s\tremaining: 15s\n",
      "688:\tlearn: 0.0143314\ttotal: 33.2s\tremaining: 15s\n",
      "689:\tlearn: 0.0143187\ttotal: 33.2s\tremaining: 14.9s\n",
      "690:\tlearn: 0.0143083\ttotal: 33.3s\tremaining: 14.9s\n",
      "691:\tlearn: 0.0142997\ttotal: 33.4s\tremaining: 14.9s\n",
      "692:\tlearn: 0.0142845\ttotal: 33.5s\tremaining: 14.8s\n",
      "693:\tlearn: 0.0142675\ttotal: 33.5s\tremaining: 14.8s\n",
      "694:\tlearn: 0.0142517\ttotal: 33.7s\tremaining: 14.8s\n",
      "695:\tlearn: 0.0142439\ttotal: 33.7s\tremaining: 14.7s\n",
      "696:\tlearn: 0.0142345\ttotal: 33.8s\tremaining: 14.7s\n",
      "697:\tlearn: 0.0142241\ttotal: 33.8s\tremaining: 14.6s\n",
      "698:\tlearn: 0.0142090\ttotal: 33.9s\tremaining: 14.6s\n",
      "699:\tlearn: 0.0141978\ttotal: 33.9s\tremaining: 14.5s\n",
      "700:\tlearn: 0.0141835\ttotal: 33.9s\tremaining: 14.5s\n",
      "701:\tlearn: 0.0141692\ttotal: 34s\tremaining: 14.4s\n",
      "702:\tlearn: 0.0141538\ttotal: 34s\tremaining: 14.4s\n",
      "703:\tlearn: 0.0141449\ttotal: 34.1s\tremaining: 14.3s\n",
      "704:\tlearn: 0.0141326\ttotal: 34.1s\tremaining: 14.3s\n",
      "705:\tlearn: 0.0141240\ttotal: 34.1s\tremaining: 14.2s\n",
      "706:\tlearn: 0.0141155\ttotal: 34.2s\tremaining: 14.2s\n",
      "707:\tlearn: 0.0141004\ttotal: 34.2s\tremaining: 14.1s\n",
      "708:\tlearn: 0.0140864\ttotal: 34.3s\tremaining: 14.1s\n",
      "709:\tlearn: 0.0140781\ttotal: 34.3s\tremaining: 14s\n",
      "710:\tlearn: 0.0140662\ttotal: 34.4s\tremaining: 14s\n",
      "711:\tlearn: 0.0140627\ttotal: 34.4s\tremaining: 13.9s\n",
      "712:\tlearn: 0.0140529\ttotal: 34.4s\tremaining: 13.9s\n",
      "713:\tlearn: 0.0140379\ttotal: 34.5s\tremaining: 13.8s\n",
      "714:\tlearn: 0.0140276\ttotal: 34.5s\tremaining: 13.8s\n",
      "715:\tlearn: 0.0140160\ttotal: 34.5s\tremaining: 13.7s\n",
      "716:\tlearn: 0.0140031\ttotal: 34.6s\tremaining: 13.7s\n",
      "717:\tlearn: 0.0139919\ttotal: 34.6s\tremaining: 13.6s\n",
      "718:\tlearn: 0.0139738\ttotal: 34.7s\tremaining: 13.6s\n",
      "719:\tlearn: 0.0139601\ttotal: 34.7s\tremaining: 13.5s\n",
      "720:\tlearn: 0.0139458\ttotal: 34.7s\tremaining: 13.4s\n",
      "721:\tlearn: 0.0139346\ttotal: 34.8s\tremaining: 13.4s\n",
      "722:\tlearn: 0.0139250\ttotal: 34.8s\tremaining: 13.3s\n",
      "723:\tlearn: 0.0139121\ttotal: 34.9s\tremaining: 13.3s\n",
      "724:\tlearn: 0.0138953\ttotal: 34.9s\tremaining: 13.2s\n",
      "725:\tlearn: 0.0138838\ttotal: 34.9s\tremaining: 13.2s\n",
      "726:\tlearn: 0.0138746\ttotal: 35s\tremaining: 13.1s\n",
      "727:\tlearn: 0.0138563\ttotal: 35s\tremaining: 13.1s\n",
      "728:\tlearn: 0.0138481\ttotal: 35s\tremaining: 13s\n",
      "729:\tlearn: 0.0138391\ttotal: 35.1s\tremaining: 13s\n",
      "730:\tlearn: 0.0138288\ttotal: 35.1s\tremaining: 12.9s\n",
      "731:\tlearn: 0.0138097\ttotal: 35.1s\tremaining: 12.9s\n",
      "732:\tlearn: 0.0137975\ttotal: 35.2s\tremaining: 12.8s\n",
      "733:\tlearn: 0.0137842\ttotal: 35.2s\tremaining: 12.8s\n",
      "734:\tlearn: 0.0137719\ttotal: 35.3s\tremaining: 12.7s\n",
      "735:\tlearn: 0.0137603\ttotal: 35.3s\tremaining: 12.7s\n",
      "736:\tlearn: 0.0137515\ttotal: 35.4s\tremaining: 12.6s\n",
      "737:\tlearn: 0.0137369\ttotal: 35.4s\tremaining: 12.6s\n",
      "738:\tlearn: 0.0137241\ttotal: 35.4s\tremaining: 12.5s\n",
      "739:\tlearn: 0.0137188\ttotal: 35.5s\tremaining: 12.5s\n",
      "740:\tlearn: 0.0137097\ttotal: 35.5s\tremaining: 12.4s\n",
      "741:\tlearn: 0.0136999\ttotal: 35.6s\tremaining: 12.4s\n",
      "742:\tlearn: 0.0136856\ttotal: 35.6s\tremaining: 12.3s\n",
      "743:\tlearn: 0.0136751\ttotal: 35.6s\tremaining: 12.3s\n",
      "744:\tlearn: 0.0136650\ttotal: 35.7s\tremaining: 12.2s\n",
      "745:\tlearn: 0.0136518\ttotal: 35.7s\tremaining: 12.2s\n",
      "746:\tlearn: 0.0136390\ttotal: 35.8s\tremaining: 12.1s\n",
      "747:\tlearn: 0.0136239\ttotal: 35.8s\tremaining: 12.1s\n",
      "748:\tlearn: 0.0136123\ttotal: 35.9s\tremaining: 12s\n",
      "749:\tlearn: 0.0136022\ttotal: 35.9s\tremaining: 12s\n",
      "750:\tlearn: 0.0135892\ttotal: 36s\tremaining: 11.9s\n",
      "751:\tlearn: 0.0135778\ttotal: 36s\tremaining: 11.9s\n",
      "752:\tlearn: 0.0135600\ttotal: 36.1s\tremaining: 11.8s\n",
      "753:\tlearn: 0.0135475\ttotal: 36.1s\tremaining: 11.8s\n",
      "754:\tlearn: 0.0135367\ttotal: 36.1s\tremaining: 11.7s\n",
      "755:\tlearn: 0.0135207\ttotal: 36.2s\tremaining: 11.7s\n",
      "756:\tlearn: 0.0135110\ttotal: 36.2s\tremaining: 11.6s\n",
      "757:\tlearn: 0.0135021\ttotal: 36.2s\tremaining: 11.6s\n",
      "758:\tlearn: 0.0134877\ttotal: 36.3s\tremaining: 11.5s\n",
      "759:\tlearn: 0.0134780\ttotal: 36.3s\tremaining: 11.5s\n",
      "760:\tlearn: 0.0134667\ttotal: 36.4s\tremaining: 11.4s\n",
      "761:\tlearn: 0.0134491\ttotal: 36.4s\tremaining: 11.4s\n",
      "762:\tlearn: 0.0134365\ttotal: 36.4s\tremaining: 11.3s\n",
      "763:\tlearn: 0.0134270\ttotal: 36.5s\tremaining: 11.3s\n",
      "764:\tlearn: 0.0134193\ttotal: 36.5s\tremaining: 11.2s\n",
      "765:\tlearn: 0.0134041\ttotal: 36.5s\tremaining: 11.2s\n",
      "766:\tlearn: 0.0133933\ttotal: 36.6s\tremaining: 11.1s\n",
      "767:\tlearn: 0.0133772\ttotal: 36.6s\tremaining: 11.1s\n",
      "768:\tlearn: 0.0133609\ttotal: 36.6s\tremaining: 11s\n",
      "769:\tlearn: 0.0133468\ttotal: 36.7s\tremaining: 11s\n",
      "770:\tlearn: 0.0133396\ttotal: 36.7s\tremaining: 10.9s\n",
      "771:\tlearn: 0.0133280\ttotal: 36.7s\tremaining: 10.9s\n",
      "772:\tlearn: 0.0133144\ttotal: 36.8s\tremaining: 10.8s\n",
      "773:\tlearn: 0.0132998\ttotal: 36.8s\tremaining: 10.8s\n",
      "774:\tlearn: 0.0132846\ttotal: 36.9s\tremaining: 10.7s\n",
      "775:\tlearn: 0.0132777\ttotal: 36.9s\tremaining: 10.6s\n",
      "776:\tlearn: 0.0132582\ttotal: 36.9s\tremaining: 10.6s\n",
      "777:\tlearn: 0.0132425\ttotal: 37s\tremaining: 10.5s\n",
      "778:\tlearn: 0.0132341\ttotal: 37s\tremaining: 10.5s\n",
      "779:\tlearn: 0.0132193\ttotal: 37s\tremaining: 10.4s\n",
      "780:\tlearn: 0.0132074\ttotal: 37.1s\tremaining: 10.4s\n",
      "781:\tlearn: 0.0131980\ttotal: 37.1s\tremaining: 10.3s\n",
      "782:\tlearn: 0.0131844\ttotal: 37.1s\tremaining: 10.3s\n",
      "783:\tlearn: 0.0131723\ttotal: 37.2s\tremaining: 10.2s\n",
      "784:\tlearn: 0.0131580\ttotal: 37.2s\tremaining: 10.2s\n",
      "785:\tlearn: 0.0131469\ttotal: 37.3s\tremaining: 10.1s\n",
      "786:\tlearn: 0.0131348\ttotal: 37.3s\tremaining: 10.1s\n",
      "787:\tlearn: 0.0131241\ttotal: 37.3s\tremaining: 10s\n",
      "788:\tlearn: 0.0131153\ttotal: 37.4s\tremaining: 9.99s\n",
      "789:\tlearn: 0.0130995\ttotal: 37.4s\tremaining: 9.94s\n",
      "790:\tlearn: 0.0130881\ttotal: 37.4s\tremaining: 9.89s\n",
      "791:\tlearn: 0.0130765\ttotal: 37.5s\tremaining: 9.84s\n",
      "792:\tlearn: 0.0130705\ttotal: 37.5s\tremaining: 9.79s\n",
      "793:\tlearn: 0.0130519\ttotal: 37.5s\tremaining: 9.74s\n",
      "794:\tlearn: 0.0130393\ttotal: 37.6s\tremaining: 9.69s\n",
      "795:\tlearn: 0.0130341\ttotal: 37.6s\tremaining: 9.64s\n",
      "796:\tlearn: 0.0130270\ttotal: 37.7s\tremaining: 9.59s\n",
      "797:\tlearn: 0.0130149\ttotal: 37.7s\tremaining: 9.54s\n",
      "798:\tlearn: 0.0130054\ttotal: 37.7s\tremaining: 9.49s\n",
      "799:\tlearn: 0.0129946\ttotal: 37.8s\tremaining: 9.44s\n",
      "800:\tlearn: 0.0129803\ttotal: 37.8s\tremaining: 9.4s\n",
      "801:\tlearn: 0.0129678\ttotal: 37.9s\tremaining: 9.35s\n",
      "802:\tlearn: 0.0129581\ttotal: 37.9s\tremaining: 9.3s\n",
      "803:\tlearn: 0.0129510\ttotal: 38s\tremaining: 9.26s\n",
      "804:\tlearn: 0.0129359\ttotal: 38s\tremaining: 9.21s\n",
      "805:\tlearn: 0.0129274\ttotal: 38.1s\tremaining: 9.16s\n",
      "806:\tlearn: 0.0129208\ttotal: 38.1s\tremaining: 9.11s\n",
      "807:\tlearn: 0.0129102\ttotal: 38.2s\tremaining: 9.07s\n",
      "808:\tlearn: 0.0129007\ttotal: 38.2s\tremaining: 9.02s\n",
      "809:\tlearn: 0.0128842\ttotal: 38.3s\tremaining: 8.97s\n",
      "810:\tlearn: 0.0128682\ttotal: 38.3s\tremaining: 8.92s\n",
      "811:\tlearn: 0.0128554\ttotal: 38.3s\tremaining: 8.88s\n",
      "812:\tlearn: 0.0128437\ttotal: 38.4s\tremaining: 8.83s\n",
      "813:\tlearn: 0.0128328\ttotal: 38.4s\tremaining: 8.78s\n",
      "814:\tlearn: 0.0128241\ttotal: 38.5s\tremaining: 8.73s\n",
      "815:\tlearn: 0.0128142\ttotal: 38.5s\tremaining: 8.68s\n",
      "816:\tlearn: 0.0127982\ttotal: 38.5s\tremaining: 8.63s\n",
      "817:\tlearn: 0.0127871\ttotal: 38.6s\tremaining: 8.58s\n",
      "818:\tlearn: 0.0127739\ttotal: 38.6s\tremaining: 8.53s\n",
      "819:\tlearn: 0.0127609\ttotal: 38.7s\tremaining: 8.48s\n",
      "820:\tlearn: 0.0127513\ttotal: 38.7s\tremaining: 8.43s\n",
      "821:\tlearn: 0.0127504\ttotal: 38.7s\tremaining: 8.39s\n",
      "822:\tlearn: 0.0127365\ttotal: 38.8s\tremaining: 8.34s\n",
      "823:\tlearn: 0.0127257\ttotal: 38.8s\tremaining: 8.29s\n",
      "824:\tlearn: 0.0127141\ttotal: 38.9s\tremaining: 8.24s\n",
      "825:\tlearn: 0.0127043\ttotal: 38.9s\tremaining: 8.2s\n",
      "826:\tlearn: 0.0126946\ttotal: 39s\tremaining: 8.15s\n",
      "827:\tlearn: 0.0126880\ttotal: 39s\tremaining: 8.1s\n",
      "828:\tlearn: 0.0126792\ttotal: 39.1s\tremaining: 8.05s\n",
      "829:\tlearn: 0.0126708\ttotal: 39.1s\tremaining: 8.01s\n",
      "830:\tlearn: 0.0126620\ttotal: 39.2s\tremaining: 7.96s\n",
      "831:\tlearn: 0.0126530\ttotal: 39.2s\tremaining: 7.92s\n",
      "832:\tlearn: 0.0126459\ttotal: 39.3s\tremaining: 7.88s\n",
      "833:\tlearn: 0.0126300\ttotal: 39.4s\tremaining: 7.83s\n",
      "834:\tlearn: 0.0126169\ttotal: 39.4s\tremaining: 7.79s\n",
      "835:\tlearn: 0.0126073\ttotal: 39.5s\tremaining: 7.74s\n",
      "836:\tlearn: 0.0125894\ttotal: 39.5s\tremaining: 7.7s\n",
      "837:\tlearn: 0.0125794\ttotal: 39.6s\tremaining: 7.65s\n",
      "838:\tlearn: 0.0125684\ttotal: 39.6s\tremaining: 7.6s\n",
      "839:\tlearn: 0.0125569\ttotal: 39.7s\tremaining: 7.57s\n",
      "840:\tlearn: 0.0125463\ttotal: 39.8s\tremaining: 7.53s\n",
      "841:\tlearn: 0.0125372\ttotal: 39.9s\tremaining: 7.48s\n",
      "842:\tlearn: 0.0125248\ttotal: 39.9s\tremaining: 7.43s\n",
      "843:\tlearn: 0.0125157\ttotal: 40s\tremaining: 7.38s\n",
      "844:\tlearn: 0.0125063\ttotal: 40s\tremaining: 7.33s\n",
      "845:\tlearn: 0.0124912\ttotal: 40s\tremaining: 7.29s\n",
      "846:\tlearn: 0.0124801\ttotal: 40.1s\tremaining: 7.24s\n",
      "847:\tlearn: 0.0124672\ttotal: 40.1s\tremaining: 7.19s\n",
      "848:\tlearn: 0.0124590\ttotal: 40.1s\tremaining: 7.14s\n",
      "849:\tlearn: 0.0124443\ttotal: 40.2s\tremaining: 7.09s\n",
      "850:\tlearn: 0.0124337\ttotal: 40.2s\tremaining: 7.04s\n",
      "851:\tlearn: 0.0124261\ttotal: 40.2s\tremaining: 6.99s\n",
      "852:\tlearn: 0.0124107\ttotal: 40.3s\tremaining: 6.94s\n",
      "853:\tlearn: 0.0124043\ttotal: 40.3s\tremaining: 6.89s\n",
      "854:\tlearn: 0.0123927\ttotal: 40.4s\tremaining: 6.84s\n",
      "855:\tlearn: 0.0123889\ttotal: 40.4s\tremaining: 6.79s\n",
      "856:\tlearn: 0.0123800\ttotal: 40.4s\tremaining: 6.75s\n",
      "857:\tlearn: 0.0123710\ttotal: 40.5s\tremaining: 6.7s\n",
      "858:\tlearn: 0.0123599\ttotal: 40.5s\tremaining: 6.65s\n",
      "859:\tlearn: 0.0123472\ttotal: 40.5s\tremaining: 6.6s\n",
      "860:\tlearn: 0.0123332\ttotal: 40.6s\tremaining: 6.55s\n",
      "861:\tlearn: 0.0123214\ttotal: 40.6s\tremaining: 6.51s\n",
      "862:\tlearn: 0.0123144\ttotal: 40.7s\tremaining: 6.46s\n",
      "863:\tlearn: 0.0123070\ttotal: 40.7s\tremaining: 6.41s\n",
      "864:\tlearn: 0.0122899\ttotal: 40.8s\tremaining: 6.36s\n",
      "865:\tlearn: 0.0122784\ttotal: 40.8s\tremaining: 6.32s\n",
      "866:\tlearn: 0.0122670\ttotal: 40.9s\tremaining: 6.27s\n",
      "867:\tlearn: 0.0122571\ttotal: 40.9s\tremaining: 6.22s\n",
      "868:\tlearn: 0.0122469\ttotal: 40.9s\tremaining: 6.17s\n",
      "869:\tlearn: 0.0122353\ttotal: 41s\tremaining: 6.12s\n",
      "870:\tlearn: 0.0122221\ttotal: 41s\tremaining: 6.07s\n",
      "871:\tlearn: 0.0122153\ttotal: 41.1s\tremaining: 6.03s\n",
      "872:\tlearn: 0.0122049\ttotal: 41.1s\tremaining: 5.98s\n",
      "873:\tlearn: 0.0121939\ttotal: 41.1s\tremaining: 5.93s\n",
      "874:\tlearn: 0.0121869\ttotal: 41.2s\tremaining: 5.88s\n",
      "875:\tlearn: 0.0121755\ttotal: 41.2s\tremaining: 5.83s\n",
      "876:\tlearn: 0.0121645\ttotal: 41.2s\tremaining: 5.78s\n",
      "877:\tlearn: 0.0121530\ttotal: 41.3s\tremaining: 5.74s\n",
      "878:\tlearn: 0.0121387\ttotal: 41.3s\tremaining: 5.69s\n",
      "879:\tlearn: 0.0121290\ttotal: 41.3s\tremaining: 5.64s\n",
      "880:\tlearn: 0.0121182\ttotal: 41.4s\tremaining: 5.59s\n",
      "881:\tlearn: 0.0121069\ttotal: 41.4s\tremaining: 5.54s\n",
      "882:\tlearn: 0.0120972\ttotal: 41.5s\tremaining: 5.49s\n",
      "883:\tlearn: 0.0120868\ttotal: 41.5s\tremaining: 5.45s\n",
      "884:\tlearn: 0.0120750\ttotal: 41.5s\tremaining: 5.4s\n",
      "885:\tlearn: 0.0120615\ttotal: 41.6s\tremaining: 5.35s\n",
      "886:\tlearn: 0.0120508\ttotal: 41.6s\tremaining: 5.3s\n",
      "887:\tlearn: 0.0120441\ttotal: 41.6s\tremaining: 5.25s\n",
      "888:\tlearn: 0.0120385\ttotal: 41.7s\tremaining: 5.2s\n",
      "889:\tlearn: 0.0120263\ttotal: 41.7s\tremaining: 5.16s\n",
      "890:\tlearn: 0.0120163\ttotal: 41.7s\tremaining: 5.11s\n",
      "891:\tlearn: 0.0120060\ttotal: 41.8s\tremaining: 5.06s\n",
      "892:\tlearn: 0.0119985\ttotal: 41.8s\tremaining: 5.01s\n",
      "893:\tlearn: 0.0119846\ttotal: 41.9s\tremaining: 4.96s\n",
      "894:\tlearn: 0.0119719\ttotal: 41.9s\tremaining: 4.91s\n",
      "895:\tlearn: 0.0119640\ttotal: 41.9s\tremaining: 4.87s\n",
      "896:\tlearn: 0.0119562\ttotal: 42s\tremaining: 4.82s\n",
      "897:\tlearn: 0.0119439\ttotal: 42s\tremaining: 4.77s\n",
      "898:\tlearn: 0.0119312\ttotal: 42s\tremaining: 4.72s\n",
      "899:\tlearn: 0.0119162\ttotal: 42.1s\tremaining: 4.67s\n",
      "900:\tlearn: 0.0119036\ttotal: 42.1s\tremaining: 4.63s\n",
      "901:\tlearn: 0.0118930\ttotal: 42.1s\tremaining: 4.58s\n",
      "902:\tlearn: 0.0118763\ttotal: 42.2s\tremaining: 4.53s\n",
      "903:\tlearn: 0.0118650\ttotal: 42.2s\tremaining: 4.48s\n",
      "904:\tlearn: 0.0118570\ttotal: 42.3s\tremaining: 4.43s\n",
      "905:\tlearn: 0.0118425\ttotal: 42.3s\tremaining: 4.39s\n",
      "906:\tlearn: 0.0118325\ttotal: 42.3s\tremaining: 4.34s\n",
      "907:\tlearn: 0.0118175\ttotal: 42.4s\tremaining: 4.29s\n",
      "908:\tlearn: 0.0118111\ttotal: 42.4s\tremaining: 4.25s\n",
      "909:\tlearn: 0.0118016\ttotal: 42.4s\tremaining: 4.2s\n",
      "910:\tlearn: 0.0117882\ttotal: 42.5s\tremaining: 4.15s\n",
      "911:\tlearn: 0.0117746\ttotal: 42.5s\tremaining: 4.1s\n",
      "912:\tlearn: 0.0117607\ttotal: 42.6s\tremaining: 4.05s\n",
      "913:\tlearn: 0.0117490\ttotal: 42.6s\tremaining: 4.01s\n",
      "914:\tlearn: 0.0117316\ttotal: 42.6s\tremaining: 3.96s\n",
      "915:\tlearn: 0.0117240\ttotal: 42.7s\tremaining: 3.91s\n",
      "916:\tlearn: 0.0117117\ttotal: 42.7s\tremaining: 3.87s\n",
      "917:\tlearn: 0.0116980\ttotal: 42.7s\tremaining: 3.82s\n",
      "918:\tlearn: 0.0116906\ttotal: 42.8s\tremaining: 3.77s\n",
      "919:\tlearn: 0.0116824\ttotal: 42.8s\tremaining: 3.72s\n",
      "920:\tlearn: 0.0116719\ttotal: 42.9s\tremaining: 3.68s\n",
      "921:\tlearn: 0.0116596\ttotal: 42.9s\tremaining: 3.63s\n",
      "922:\tlearn: 0.0116544\ttotal: 43s\tremaining: 3.58s\n",
      "923:\tlearn: 0.0116422\ttotal: 43s\tremaining: 3.54s\n",
      "924:\tlearn: 0.0116341\ttotal: 43.1s\tremaining: 3.49s\n",
      "925:\tlearn: 0.0116240\ttotal: 43.1s\tremaining: 3.44s\n",
      "926:\tlearn: 0.0116151\ttotal: 43.1s\tremaining: 3.4s\n",
      "927:\tlearn: 0.0116073\ttotal: 43.2s\tremaining: 3.35s\n",
      "928:\tlearn: 0.0115978\ttotal: 43.2s\tremaining: 3.3s\n",
      "929:\tlearn: 0.0115899\ttotal: 43.3s\tremaining: 3.26s\n",
      "930:\tlearn: 0.0115835\ttotal: 43.3s\tremaining: 3.21s\n",
      "931:\tlearn: 0.0115691\ttotal: 43.4s\tremaining: 3.16s\n",
      "932:\tlearn: 0.0115615\ttotal: 43.4s\tremaining: 3.12s\n",
      "933:\tlearn: 0.0115536\ttotal: 43.4s\tremaining: 3.07s\n",
      "934:\tlearn: 0.0115427\ttotal: 43.5s\tremaining: 3.02s\n",
      "935:\tlearn: 0.0115323\ttotal: 43.5s\tremaining: 2.98s\n",
      "936:\tlearn: 0.0115306\ttotal: 43.6s\tremaining: 2.93s\n",
      "937:\tlearn: 0.0115252\ttotal: 43.6s\tremaining: 2.88s\n",
      "938:\tlearn: 0.0115144\ttotal: 43.6s\tremaining: 2.83s\n",
      "939:\tlearn: 0.0115056\ttotal: 43.7s\tremaining: 2.79s\n",
      "940:\tlearn: 0.0114956\ttotal: 43.7s\tremaining: 2.74s\n",
      "941:\tlearn: 0.0114832\ttotal: 43.8s\tremaining: 2.69s\n",
      "942:\tlearn: 0.0114702\ttotal: 43.8s\tremaining: 2.65s\n",
      "943:\tlearn: 0.0114562\ttotal: 43.9s\tremaining: 2.6s\n",
      "944:\tlearn: 0.0114403\ttotal: 43.9s\tremaining: 2.55s\n",
      "945:\tlearn: 0.0114325\ttotal: 43.9s\tremaining: 2.51s\n",
      "946:\tlearn: 0.0114227\ttotal: 44s\tremaining: 2.46s\n",
      "947:\tlearn: 0.0114133\ttotal: 44s\tremaining: 2.42s\n",
      "948:\tlearn: 0.0114031\ttotal: 44.1s\tremaining: 2.37s\n",
      "949:\tlearn: 0.0113908\ttotal: 44.1s\tremaining: 2.32s\n",
      "950:\tlearn: 0.0113830\ttotal: 44.2s\tremaining: 2.27s\n",
      "951:\tlearn: 0.0113753\ttotal: 44.2s\tremaining: 2.23s\n",
      "952:\tlearn: 0.0113631\ttotal: 44.3s\tremaining: 2.18s\n",
      "953:\tlearn: 0.0113543\ttotal: 44.3s\tremaining: 2.13s\n",
      "954:\tlearn: 0.0113443\ttotal: 44.3s\tremaining: 2.09s\n",
      "955:\tlearn: 0.0113363\ttotal: 44.4s\tremaining: 2.04s\n",
      "956:\tlearn: 0.0113245\ttotal: 44.4s\tremaining: 2s\n",
      "957:\tlearn: 0.0113130\ttotal: 44.5s\tremaining: 1.95s\n",
      "958:\tlearn: 0.0113016\ttotal: 44.5s\tremaining: 1.9s\n",
      "959:\tlearn: 0.0112913\ttotal: 44.5s\tremaining: 1.85s\n",
      "960:\tlearn: 0.0112829\ttotal: 44.6s\tremaining: 1.81s\n",
      "961:\tlearn: 0.0112747\ttotal: 44.6s\tremaining: 1.76s\n",
      "962:\tlearn: 0.0112589\ttotal: 44.7s\tremaining: 1.72s\n",
      "963:\tlearn: 0.0112514\ttotal: 44.7s\tremaining: 1.67s\n",
      "964:\tlearn: 0.0112424\ttotal: 44.7s\tremaining: 1.62s\n",
      "965:\tlearn: 0.0112367\ttotal: 44.8s\tremaining: 1.58s\n",
      "966:\tlearn: 0.0112240\ttotal: 44.8s\tremaining: 1.53s\n",
      "967:\tlearn: 0.0112146\ttotal: 44.9s\tremaining: 1.48s\n",
      "968:\tlearn: 0.0112055\ttotal: 44.9s\tremaining: 1.44s\n",
      "969:\tlearn: 0.0111953\ttotal: 44.9s\tremaining: 1.39s\n",
      "970:\tlearn: 0.0111891\ttotal: 45s\tremaining: 1.34s\n",
      "971:\tlearn: 0.0111759\ttotal: 45s\tremaining: 1.3s\n",
      "972:\tlearn: 0.0111671\ttotal: 45.1s\tremaining: 1.25s\n",
      "973:\tlearn: 0.0111565\ttotal: 45.1s\tremaining: 1.2s\n",
      "974:\tlearn: 0.0111450\ttotal: 45.1s\tremaining: 1.16s\n",
      "975:\tlearn: 0.0111369\ttotal: 45.2s\tremaining: 1.11s\n",
      "976:\tlearn: 0.0111272\ttotal: 45.2s\tremaining: 1.06s\n",
      "977:\tlearn: 0.0111146\ttotal: 45.3s\tremaining: 1.02s\n",
      "978:\tlearn: 0.0111045\ttotal: 45.3s\tremaining: 972ms\n",
      "979:\tlearn: 0.0110961\ttotal: 45.3s\tremaining: 925ms\n",
      "980:\tlearn: 0.0110855\ttotal: 45.4s\tremaining: 879ms\n",
      "981:\tlearn: 0.0110778\ttotal: 45.4s\tremaining: 832ms\n",
      "982:\tlearn: 0.0110637\ttotal: 45.4s\tremaining: 786ms\n",
      "983:\tlearn: 0.0110501\ttotal: 45.5s\tremaining: 740ms\n",
      "984:\tlearn: 0.0110376\ttotal: 45.5s\tremaining: 693ms\n",
      "985:\tlearn: 0.0110286\ttotal: 45.6s\tremaining: 647ms\n",
      "986:\tlearn: 0.0110159\ttotal: 45.6s\tremaining: 601ms\n",
      "987:\tlearn: 0.0110056\ttotal: 45.7s\tremaining: 555ms\n",
      "988:\tlearn: 0.0109935\ttotal: 45.7s\tremaining: 508ms\n",
      "989:\tlearn: 0.0109865\ttotal: 45.7s\tremaining: 462ms\n",
      "990:\tlearn: 0.0109754\ttotal: 45.8s\tremaining: 416ms\n",
      "991:\tlearn: 0.0109628\ttotal: 45.8s\tremaining: 370ms\n",
      "992:\tlearn: 0.0109488\ttotal: 45.9s\tremaining: 323ms\n",
      "993:\tlearn: 0.0109389\ttotal: 45.9s\tremaining: 277ms\n",
      "994:\tlearn: 0.0109310\ttotal: 46s\tremaining: 231ms\n",
      "995:\tlearn: 0.0109166\ttotal: 46s\tremaining: 185ms\n",
      "996:\tlearn: 0.0109078\ttotal: 46.1s\tremaining: 139ms\n",
      "997:\tlearn: 0.0109014\ttotal: 46.1s\tremaining: 92.4ms\n",
      "998:\tlearn: 0.0108933\ttotal: 46.2s\tremaining: 46.2ms\n",
      "999:\tlearn: 0.0108857\ttotal: 46.2s\tremaining: 0us\n",
      "R2 score: -0.1027\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "def catboost(X, y, df_results, df_feature_importances):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X[:-252], X[-252:], y[:-252], y[-252:]\n",
    "\n",
    "    # Training CatBoost on the Training set\n",
    "    from catboost import CatBoostRegressor\n",
    "    regressor = CatBoostRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    df_results['cat_y_pred'] = regressor.predict(X_test)\n",
    "\n",
    "    # Evaluating the Model Performance\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(\"R2 score: {:.4f}\".format(r2_score(y_test, df_results['cat_y_pred'])))\n",
    "\n",
    "    # store feature importance\n",
    "    df_feature_importances['cat'] = regressor.feature_importances_\n",
    "\n",
    "    return df_results\n",
    "\n",
    "df_results = catboost(X, y, df_results, df_feature_importances)\n",
    "\n",
    "# R2 score: -0.0366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mlr</th>\n",
       "      <th>dtr</th>\n",
       "      <th>rfr</th>\n",
       "      <th>svr</th>\n",
       "      <th>xgb</th>\n",
       "      <th>cat</th>\n",
       "      <th>mlr_op</th>\n",
       "      <th>dtr_op</th>\n",
       "      <th>rfr_op</th>\n",
       "      <th>svr_op</th>\n",
       "      <th>xgb_op</th>\n",
       "      <th>cat_op</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Correct sign predictions over total number of predictions</th>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>46.43 %</td>\n",
       "      <td>54.37 %</td>\n",
       "      <td>46.83 %</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>137</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct up predictions over total number of up predictions</th>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>54.31 %</td>\n",
       "      <td>52.27 %</td>\n",
       "      <td>54.43 %</td>\n",
       "      <td>51.85 %</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>126</td>\n",
       "      <td>23</td>\n",
       "      <td>129</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct up predictions over total number of up predictions 0.3</th>\n",
       "      <td>nan %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>55.56 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct up predictions over total number of up predictions 0.5</th>\n",
       "      <td>nan %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>55.56 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct up predictions over total number of up predictions 0.7</th>\n",
       "      <td>nan %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>75.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct up predictions over total number of up predictions 1</th>\n",
       "      <td>nan %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>50.00 %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct up predictions over total number of up predictions 1+</th>\n",
       "      <td>nan %</td>\n",
       "      <td>42.86 %</td>\n",
       "      <td>61.90 %</td>\n",
       "      <td>nan %</td>\n",
       "      <td>33.33 %</td>\n",
       "      <td>100.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(BENCHMARK) Ups over total moves (informs about the data, not the prediction)</th>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>53.97 %</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        mlr      dtr  \\\n",
       "Correct sign predictions over total number of p...  53.97 %  53.97 %   \n",
       "Correct up predictions over total number of up ...  53.97 %  53.97 %   \n",
       "Correct up predictions over total number of up ...    nan %    nan %   \n",
       "Correct up predictions over total number of up ...    nan %    nan %   \n",
       "Correct up predictions over total number of up ...    nan %    nan %   \n",
       "Correct up predictions over total number of up ...    nan %    nan %   \n",
       "Correct up predictions over total number of up ...    nan %  42.86 %   \n",
       "(BENCHMARK) Ups over total moves (informs about...  53.97 %  53.97 %   \n",
       "\n",
       "                                                         rfr       svr  \\\n",
       "Correct sign predictions over total number of p...   53.97 %   46.43 %   \n",
       "Correct up predictions over total number of up ...   54.31 %   52.27 %   \n",
       "Correct up predictions over total number of up ...     nan %  100.00 %   \n",
       "Correct up predictions over total number of up ...  100.00 %  100.00 %   \n",
       "Correct up predictions over total number of up ...     nan %  100.00 %   \n",
       "Correct up predictions over total number of up ...   50.00 %     nan %   \n",
       "Correct up predictions over total number of up ...   61.90 %     nan %   \n",
       "(BENCHMARK) Ups over total moves (informs about...   53.97 %   53.97 %   \n",
       "\n",
       "                                                        xgb       cat  mlr_op  \\\n",
       "Correct sign predictions over total number of p...  54.37 %   46.83 %     136   \n",
       "Correct up predictions over total number of up ...  54.43 %   51.85 %     136   \n",
       "Correct up predictions over total number of up ...    nan %   55.56 %       0   \n",
       "Correct up predictions over total number of up ...    nan %   55.56 %       0   \n",
       "Correct up predictions over total number of up ...    nan %   75.00 %       0   \n",
       "Correct up predictions over total number of up ...    nan %    0.00 %       0   \n",
       "Correct up predictions over total number of up ...  33.33 %  100.00 %       0   \n",
       "(BENCHMARK) Ups over total moves (informs about...  53.97 %   53.97 %     136   \n",
       "\n",
       "                                                    dtr_op  rfr_op  svr_op  \\\n",
       "Correct sign predictions over total number of p...     136     136     117   \n",
       "Correct up predictions over total number of up ...     136     126      23   \n",
       "Correct up predictions over total number of up ...       0       0       3   \n",
       "Correct up predictions over total number of up ...       0       1       2   \n",
       "Correct up predictions over total number of up ...       0       0       1   \n",
       "Correct up predictions over total number of up ...       0       1       0   \n",
       "Correct up predictions over total number of up ...       9      13       0   \n",
       "(BENCHMARK) Ups over total moves (informs about...     136     136     136   \n",
       "\n",
       "                                                    xgb_op  cat_op  \n",
       "Correct sign predictions over total number of p...     137     118  \n",
       "Correct up predictions over total number of up ...     129      42  \n",
       "Correct up predictions over total number of up ...       0       5  \n",
       "Correct up predictions over total number of up ...       0       5  \n",
       "Correct up predictions over total number of up ...       0       3  \n",
       "Correct up predictions over total number of up ...       0       0  \n",
       "Correct up predictions over total number of up ...       1       1  \n",
       "(BENCHMARK) Ups over total moves (informs about...     136     136  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame with different metrics of success for each model\n",
    "df_success = pd.DataFrame(columns=['mlr', 'dtr', 'rfr', 'svr', 'xgb', 'cat'],\n",
    "                            index=['Correct sign predictions over total number of predictions', \n",
    "                                    'Correct up predictions over total number of up predictions', \n",
    "                                    'Correct up predictions over total number of up predictions 0.3',\n",
    "                                    'Correct up predictions over total number of up predictions 0.5',\n",
    "                                    'Correct up predictions over total number of up predictions 0.7',\n",
    "                                    'Correct up predictions over total number of up predictions 1',\n",
    "                                    'Correct up predictions over total number of up predictions 1+',\n",
    "                                    '(BENCHMARK) Ups over total moves (informs about the data, not the prediction)'])\n",
    "\n",
    "def get_success_rate(df_results, model):\n",
    "    number_of_op = []\n",
    "    # get the ratio of the correct sign predictions to the total number of predictions\n",
    "    same_sign = (np.sign(df_results[model + '_y_pred']) == np.sign(df_results['y_test']))\n",
    "    number_of_op.append(np.where(same_sign, 1, 0).sum())\n",
    "    success_rate = number_of_op[0] / df_results.shape[0]\n",
    "    df_success[model][0] = \"{:.2f} %\".format(success_rate*100)\n",
    "    # get the ratio of the correct up predictions to the total number of up predictions\n",
    "    number_of_op.append(np.where((same_sign) & (df_results[model + '_y_pred'] > 0), 1, 0).sum())\n",
    "    success_rate = number_of_op[1] / np.where(df_results[model + '_y_pred'] > 0, 1, 0).sum()\n",
    "    df_success[model][1] = \"{:.2f} %\".format(success_rate*100)\n",
    "    # get the ratio of the correct up predictions to the total number of up predictions when the up prediction is greater than 5%\n",
    "    lst = [0, 0.3, 0.5, 0.7, 1, np.inf]\n",
    "    for i in range(1, len(lst)):\n",
    "        mean, std = df_results[model + '_y_pred'].abs().mean(), df_results[model + '_y_pred'].std()\n",
    "        upper_bond, lower_bond = mean + lst[i] * std, mean + lst[i-1] * std\n",
    "        above_lower = (df_results[model + '_y_pred'] > lower_bond)\n",
    "        below_upper = (df_results[model + '_y_pred'] < upper_bond)\n",
    "        number_of_op.append(np.where((above_lower) & (below_upper) & (same_sign), 1, 0).sum())\n",
    "        success_rate = number_of_op[-1] / np.where(((above_lower) & (below_upper)), 1, 0).sum()\n",
    "        df_success[model][1 + i] = \"{:.2f} %\".format(success_rate*100)\n",
    "    # get the ratio of the ups to the total number of moves\n",
    "    number_of_op.append(np.where(df_results['y_test'] > 0, 1, 0).sum())\n",
    "    success_rate = number_of_op[1 + len(lst)] / df_results.shape[0]\n",
    "    df_success[model][1 + len(lst)] = \"{:.2f} %\".format(success_rate*100)\n",
    "    df_success[model + '_op'] = number_of_op\n",
    "\n",
    "for model in df_success.columns:\n",
    "    get_success_rate(df_results, model)\n",
    "df_success\n",
    "\n",
    "# DataFrame without DPO feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
